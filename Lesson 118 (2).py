# -*- coding: utf-8 -*-
"""Copy of 20221223Kalyan- Lesson 118

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1zI3TPYwWRU69Hr7WQKgHAEYRBaUgfAfB

# Lesson 118: PCA - Eigenvectors and Eigenvalues

---

#### Teacher-Student Activities

In the previous class, we understood few more concepts of vectors that are vital in PCA. We also learned how eigenvectors and eigenvalues form the basis for PCA projection.

In today's class, we will learn how to calculate eigenvectors and eigenvalues. We will also execute PCA manually using eigenvectors and compare their results with `sklearn` PCA.

Before that, let us go through the concepts covered in the previous class and begin the class from the topic: **Activity 1: Understanding Eigenvectors and Eigenvalues**.

---

#### Recap

#### Loading the Dataset

**Dataset Link:**  https://s3-whjr-curriculum-uploads.whjr.online/be99ea2b-cb07-4e52-b9ee-4c7e893ae48d.csv
"""

# Import the Python modules, read the dataset and create a Pandas DataFrame
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px

# Read the dataset
wheat_df = pd.read_csv('https://s3-whjr-curriculum-uploads.whjr.online/52e55558-5ad7-4f93-a854-8186f415bc55.csv')
wheat_df.head()

# Rename the columns for better understandability
wheat_df.rename(columns = {'A': 'area', 'P': 'perimeter', 'C': 'compactness',
                            'LK': 'kernel_length', 'WK': 'kernel_width',
                            'A_Coef': 'asymmetry_coefficient', 'LKG':'kernel_groove_length'}, inplace = True)
wheat_df.head()

# Get the total number of rows and columns, data types of columns and missing values (if exist) in the dataset.
wheat_df.info()

"""---

#### Exploratory Data Analysis

1. Analysis of data where kernel width is more than kernel length.
2. Check for anomalies in kernel groove length and kernel length.
3. Calculating compactness using area and perimeter.

$Compactness = \frac{4*Π*Area}{Circumference^2}$
"""

# Check for anomalies in 'kernel_width' and 'kernel_length'.
wheat_df[wheat_df['kernel_width'] > wheat_df['kernel_length']]

# Check for anomalies in kernel_groove_length and kernel_length.
print("Number of rows having kernel groove length > kernel length =",
      wheat_df[wheat_df['kernel_groove_length'] > wheat_df['kernel_length']].shape[0], "\n") # You can add this code part later.

wheat_df[wheat_df['kernel_groove_length'] > wheat_df['kernel_length']] # Write only this part of code first. Then add the preceding part later.

# Obtain a clean DataFrame
clean_df = wheat_df[wheat_df['kernel_groove_length'] < wheat_df['kernel_length']]
clean_df

# Create a duplicate copy of the 'clean_df' DataFrame
validation_df = clean_df.copy()
validation_df.head()

# Calculate the compactness value using the given formula
import math
validation_df['compactness_formula'] = (validation_df['area'] * 4 * (math.pi)) / (validation_df['perimeter'] ** 2)
validation_df

# Check for anomalies in 'compactness' feature
validation_df[validation_df['compactness'] - validation_df['compactness_formula'] > 0.01]

"""---

#### Scaling Dataset Using `StandardScaler`
Scaling data is required to change the numeric values into a scale of data, without distorting differences in the ranges of values or losing information.
"""

# Create a DataFrame having only feature variables
wheat_features = clean_df.drop(['target'], axis = 1)
wheat_features.head()

# Normalise the column values.
from sklearn.preprocessing import StandardScaler

standard_scaler = StandardScaler()
scaled_values = standard_scaler.fit_transform(wheat_features)
wheat_scaled = pd.DataFrame(scaled_values)
wheat_scaled.columns = wheat_features.columns
wheat_scaled.head()

"""---

#### The Recipe of PCA

As you might have understood so far, PCA is all about finding Principal Components for which the steps are as follows:


<center>
<img src=https://s3-whjr-v2-prod-bucket.whjr.online/whjr-v2-prod-bucket/c64fbd2d-4017-490a-a568-ad557a3eac30.png> </center>

---

#### Covariance

**Covariance** is a statistical parameter that indicates the relationship between two features (or variables).
 - If an increase in one variable results in an increase in the other variable, then both the variables are said to have **positive** covariance.
 - If an increase or decrease in one variable results in an opposite change in the other variable, then both the variables are said to have a **negative** covariance.

Now, you might be wondering if this sounds exactly  like **correlation**. Well, not entirely. We will soon find out. Till then, to get a better understanding of both covariances, let's look at their mathematical formula.

Let $x_1$ and $x_2$ be two different features of a DataFrame. Then, mathematically, **covariance** is defined as

\begin{align}
\text{cov}(x_1, x_2) = \frac{1}{N}\sum_{i = 1} ^N (x_{1i} - \bar x_1)(x_{2i} - \bar x_2)
\end{align}

Where,

- $\bar x_1$ is the mean value of feature $x_1$
- $\bar x_2$ is the mean value of feature $x_2$
- $N$ is the total number of samples in a DataFrame
- $i$ is one of the samples in a DataFrame

---

#### Dot Product of Vectors and its Matrix Representation


Mathematically the **dot product** of two vectors $\vec a$ and $\vec b$ is defined as:

\begin{equation}
\vec a . \vec b = |\vec a| |\vec b| \text{cos} \space \theta
\tag{3.1}
\end{equation}

Where, $\theta$ is the angle between vectors $\vec a$ and $\vec b$. The result of this dot product is a scalar value. Hence, it is also commonly known as **scalar product** or **inner product** of two vectors.

**Dot product in matrix notation:**


Let us represent the vectors $\vec{a}$ and $\vec{b}$ in matrix form as follows:

\begin{matrix}  
{a} = \begin{bmatrix} a_1 \\ a_2 \\ a_3 \\ \vdots \\ a_n \end{bmatrix}_{n \times 1} &
{b} = \begin{bmatrix} b_1 \\ b_2 \\ b_3 \\ \vdots \\ b_n \end{bmatrix}_{n \times 1}
\end{matrix}  

The dot product of $\vec{a}$ and $\vec{b}$ can also be calculated as:


\begin{equation}
\vec a . \vec b = [(a_1 b_1) + (a_2 b_2) + (a_3 b_3) + \dots + (a_n b_n)]
\tag{3.2}
\end{equation}

If we try to multiply the above two vectors $\vec{a}$ and $\vec{b}$ using matrix multiplication, their product will not be defined. This is because, as per the rules of matrix multiplication, the number of columns of the first matrix must match the number of rows of the second matrix.

To rectify this problem, we can take transpose of the first matrix, turning it into a $1 n$ row matrix as follows:

\begin{matrix}  
a^T = \begin{bmatrix} a_1 & a_2 & a_3 & \cdots & a_n \end{bmatrix}_{1 \times n} &&
{b} = \begin{bmatrix} b_1 \\ b_2 \\ b_3 \\ \vdots \\ b_n \end{bmatrix}_{n \times 1}
\end{matrix}  

After taking transpose of $\vec{a}$, the product of vectors $\vec{a}$ and $\vec{b}$ is now well-defined as the number of columns of $\vec{a}$ is now equal to the number of rows of  $\vec{b}$ .


\begin{align}  
a^Tb &= \begin{bmatrix} a_1 & a_2 & a_3 & \cdots & a_n \end{bmatrix} \begin{bmatrix} b_1 \\ b_2 \\ b_3 \\ \vdots \\ b_n \end{bmatrix} \\
&= [(a_1 b_1) + (a_2 b_2) + (a_3 b_3) + \dots + (a_n b_n)]
\end{align}  

You may observe that the result of  above matrix multiplication $a^Tb$ is equivalent to the dot product of $\vec{a} . \vec{b}$ (Equation $3.2$).

Thus,
\begin{equation}
\boxed{\vec a . \vec b = a^Tb}
\tag{3.3}
\end{equation}

In Python, we can obtain dot product of two vectors or matrices using `dot()` function of `numpy` module.

---

#### Projection of Vectors

Let's say we have two vectors $\vec u$ and $\vec v$ and the vector $\vec u$ makes an angle $\theta$ with respect to vector $\vec v$ as shown in image below:

<center><img src=https://s3-whjr-v2-prod-bucket.whjr.online/a59e9078-e6ed-4aad-b39c-f11f9dd821d0.png width = 700></center>

Imagine a light source, parallel to $\vec v$ , above $\vec u$. The light would cast rays perpendicular or orthogonal to $\vec v$.

$g$ is the shadow cast by $\vec u$ on $\vec v$. This shadow vector is known as the **projection** of vector $\vec u$ on vector $\vec v$.

 The line segment $g$ indicates the magnitude or length of this shadow vector and is also known as the **projection** of $\vec u$ on $\vec v$ or $\text{proj}_\vec{v} \vec {u}$. Let us determine the value  of $g$.

**Projection  of $\vec u$ on $\vec v$:**

We know that as per the trigonometry ratios of an angle $θ$  in a right-angled triangle:

\begin{align}
\text{cos} \space \theta &= \frac{\text {Base}}{\text {Hypotenuse}}
\end{align}

In the above image,
- $\text{Base} = g$
- $\text{Hypotenuse} = |\vec {u}|$

Therefore,

\begin{align}
\text{cos} \space \theta &= \frac{\text {g}}{|\vec {u}|}
\end{align}

This can be rewritten as:
\begin{align}
\text {g} &= |\vec {u}| \text{cos} \space \theta \\
\tag{4.1}
\end{align}

Recall the dot product formula of two vectors (equation $(3.1)$):

\begin{align}
\vec u . \vec v &= |\vec u| |\vec v| \text{cos} \space \theta \\
\Rightarrow
\text{cos} \space \theta &= \frac{\vec u . \vec v }{|\vec u| |\vec v|}
\tag{4.2}
\end{align}

Let us substitute this value of   $\text{cos} \space \theta$ in the equation $(4.1)$.

\begin{align}
\text {g} &= |\vec {u}| \times  \frac{\vec u . \vec v }{|\vec u| |\vec v|} \\
&= \frac{\vec u . \vec v }{|\vec v|} \\
\end{align}

Thus, the formula for calculating the projection of vector $\vec u$ onto another vector $\vec v$ is:

\begin{equation}
\boxed{\text{proj}_{\vec{v} \vec {u}} = \frac{\vec u . \vec v }{|\vec v|}}
\tag{4.3}
\end{equation}


Now that we have explored all the necessary concepts needed to understand the working of PCA, let us now learn how to determine the desired principal components or vectors mathematically.

---

#### Maximising  Variance

Given the set of observations ${x_i}$  in a $d$-dimensional dataset where, $i = 1, 2, …, N$.

Our goal is to find the projection of $x_i$ onto a space with dimensions $k < d$ such that it gives maximum variance.

We’ll start by looking for a one-dimensional projection. We define a vector $u_1$ as the direction of the lower-dimensional space.

<center>
<img src="https://s3-whjr-v2-prod-bucket.whjr.online/4ebdf0ff-eafb-440d-b0e7-361f804bc732.jpg"/></center>

Since we are only interested in the direction of  this vector, we will consider $u_1$ to be a unit vector i.e. $|\vec u_1| = 1$.

The projection of a data vector $x_i$ onto
the unit vector $\vec u$ can be given as:
\begin{equation}
\text{proj}_\vec{u_1} \vec {x_i}=\frac{\vec u_1 . \vec x_i }{|\vec u_1|}
\tag{from equation 4.3 }
\end{equation}

As, $|\vec u_1| = 1$ and $\vec u_1 . \vec x_i = u_1^T.x$ (from equation $3.3$),

\begin{equation}
\text{proj}_\vec{u_1} \vec {x_i}= u_1^T.x_i
\tag{5.1}
\end{equation}

If $\bar{x}$ is the mean of the data observations $x_i$ in the original space, then the mean of $x_i$ in the projected space is given by:

\begin{equation}
\text{proj}_\vec{u_1} \bar {x}= u_1^T.\bar{x}
\tag{5.2}
\end{equation}

**Calculating variance:**

The variance of a dataset from its mean is calculated by finding the difference between each data value and the mean, squaring the differences and then finding the sum of all squared differences.

Thus, the variance of projections is given as:


\begin{align}
\text {var}(x_i) &= \frac{1}{N} \sum\limits_{i = 1}^N \bigg[\text{proj}_\vec{u_1} \vec {x_i} - \text{proj}_\vec{u_1} \bar {x}\bigg]^2 \\
&= \frac{1}{N} \sum\limits_{i = 1}^N \bigg[u_1^T.x_i - u_1^T.\bar{x}\bigg]^2 \\
&= \frac{1}{N} \sum\limits_{i = 1}^N \bigg[u_1^T .(x_i - \bar{x})\bigg]^2 \\
&= \frac{1}{N} \sum\limits_{i = 1}^N \underbrace{\bigg[u_1^T . (x_i - \bar{x})\bigg]}_\text{1st term} \underbrace{\bigg[u_1^T . (x_i - \bar{x})\bigg]}_\text{2nd term} \\ \\
\end{align}

The $2^{nd}$ term in the above equation can also be written as : $\bigg[u_1. (x_i - \bar{x})^T\bigg]$ as both $u_1$ and $(x_i - \bar{x})$ are vectors and their dot product will be same no matter in whichever order we take transpose i.e.

$\bigg[u_1^T. (x_i - \bar{x})\bigg] \equiv \bigg[u_1. (x_i - \bar{x})^T\bigg]$

Thus,

\begin{align}
\text {var}(x_i) &= \frac{1}{N} \sum\limits_{i = 1}^N \bigg[u_1^T . (x_i - \bar{x})\bigg]\bigg[u_1 . (x_i - \bar{x})^T\bigg] \\
&= u_1^T. u_1.\frac{1}{N} \sum\limits_{i = 1}^N \bigg[(x_i - \bar{x}).(x_i - \bar{x})^T \bigg] \\
&= u_1^T. S. u_1 \\
\end{align}

Where, $S$ is the  covariance matrix of the observed data in the original high dimensional space.


$S = \frac{1}{N} \sum\limits_{i = 1}^N \bigg[(x_i - \bar{x}).(x_i - \bar{x})^T \bigg]$

**Note:** Such covariance matrix is known as **closed form of covariance matrix**. We will not explore this in much detail.

Thus,
\begin{align}
\text {var}(x_i) = u_1^T. S. u_1
\end{align}

Our goal is to obtain a vector $u_1$ such that it  maximises variance of projections i.e. $\text{var}(x_i)$ with the constraint $|u_1| = 1$ or $u_1^T. u_1 = 1$.

Thus, the maximisation function looks like this:

\begin{align}
\text{max} \space && u_1^T. S. u_1 \\
\text{subject to} \space && u_1^T. u_1 = 1
\end{align}

Such problems of constrained optimization can be solved easily using **Lagrangian multipliers**.
This technique says that if we need to maximise a function $f(x)$ subject to constraint $g(x)=c$, we introduce the Lagrange multiplier $\lambda$ and construct the Lagrangian $\mathcal{L}(x, \lambda)$:

$$\mathcal{L}(x, \lambda) = f(x) - \lambda (g(x) - c)$$

In our case,
- $f(x) = u_1^T. S. u_1$
- $g(x) = u_1^T. u_1$
- $c = 1$

Thus, our Lagrangian is:

$$\mathcal{L}(u_1, \lambda_1) = u_1^T. S. u_1 - \lambda_1 (u_1^T. u_1 - 1)$$

Taking partial derivative of above lagrangian function with respect to $u_1$,

\begin{align}
\frac{\partial \mathcal{L}}{\partial u_1} = 2.S.u_1 - 2 .\lambda_1. u_1
\end{align}

Equating the above equation to $0$, we get
\begin{align}
2.S.u_1 - 2 .\lambda_1. u_1 = 0 \\
S.u_1 - \lambda_1. u_1 = 0 \\
\boxed{S.u_1 = \lambda_1. u_1} \tag{5.3}
\end{align}

This desired vector $u_1$ is nothing but an **eigenvector** of the covariance matrix $S$, and the maximum variance is equal to the **eigenvalue** $\lambda_1$. This is good news because all you need to do is find eigenvectors having the largest eigenvalue and you will get your principal component or the desired dimension.

Similarly, we can obtain additional principal components  by choosing directions that maximise variance while being orthogonal to the existing ones.

Let us now understand the concept of Eigenvectors and Eigenvalues in detail and how to calculate them using Python.

---

#### Activity 1: Understanding Eigenvectors and Eigenvalues

Let us first understand linear transformations in 1D and 2D.

**Linear Transformation in 1D Data:**

Consider the following function having one dimension $x$:

 $$f(x) = 2x + 3$$

If $x = 2$, then $f(x) = 2 \times 2 + 3 = 7$

<center>
<img src = "https://s3-whjr-v2-prod-bucket.whjr.online/363834b6-1b34-44f1-a0fd-1733cd2ec392.png"/>  

`Fig: 1D Linear Transformation`</center>

Thus, the function $f(x)$ transforms $2$ to $7$.

**Linear Transformation in 2D Data:**

For 2D data, we use a transformation matrix to transform the data items.

For example, consider a vector $\vec v = (v_x, v_y)$.

\begin{matrix}  
\overbrace{\begin{bmatrix} 2 & 1 \\ 1.5 & 2 \end{bmatrix}}^A .
\overbrace{\begin{bmatrix} 0.75 \\ 0.25 \end{bmatrix}}^\vec v =
\overbrace{\begin{bmatrix} 1.75 \\ 1.625 \end{bmatrix}}^\vec b
\end{matrix}  


In the above equation, the transformation matrix $A$ transforms a vector $\vec v$ into another vector $\vec b$.

<center>
<img src="https://s3-whjr-v2-prod-bucket.whjr.online/dc32811f-248b-4cdf-b68a-fbc05ffa0be3.png"/>

`Fig: 2D Linear Transformation` </center>

In the graph below, we can see how matrix $A$ transformed the short vector $\vec v$ into the long vector $\vec b$.

<center>
<img src="https://s3-whjr-v2-prod-bucket.whjr.online/36b35ee8-7511-479f-a8e2-ef43eb045d7a.PNG"/></center>

In this transformation, both the magnitude (length) and the direction of the original vector is changed.


Consider the vector $(2,1)$ in the following graph:
<center>
<img src="https://s3-whjr-v2-prod-bucket.whjr.online/b9de4fc0-0934-4a0d-8bae-08a802e2a00a.png"/></center>

Here, the vector $(2, 1)$ is transformed into another vector $(4, 2)$. You may observe that the linear transformation changed the magnitude of original vector but the direction of both the vectors is still the same. Such special vectors are called **Eigenvectors**.


**What are Eigenvectors?**

- An Eigenvector is a vector whose direction remains unchanged when a linear transformation is applied to it.
- Eigenvectors satisfy the following equation:
$$Av = \lambda v$$
Where,
  -  $A$ is an $(n \times n)$ transformation matrix.
  - $v$ is the Eigenvector.
  - $\lambda$ is the magnitude of the scaled vector. It is also known as **Eigenvalue**.
  - Thus, $\vec v$ will become $A \vec v$ by simply stretching $\vec v$ in the same direction as shown in the image below.

    <img src ="https://s3-whjr-v2-prod-bucket.whjr.online/da8e7e35-ca3f-4422-b99f-3f5a35e44ed0.png"/>

**Eigenvectors and Eigenvalues in PCA:**

The eigenvectors and eigenvalues of a covariance matrix is the core of PCA:
- The eigenvectors (principal components) determine the directions of the new axis.
- The eigenvalues determine their magnitude (variance explained along the principal components).

In PCA, the original axis is rotated by some degree to obtain a new axis that shows the direction of maximum variance in the dataset.
<center>
<img src=https://s3-whjr-v2-prod-bucket.whjr.online/95a92a74-7087-4798-8dc7-dae1662f6468.png width='500'></center>

This new axis (green colour axis) which covers the maximum variance in the dataset is the first eigenvector or the **first principal component**.

The second Eigenvector or second principal component will be perpendicular or orthogonal to the first one (red colour axis). The reason the two eigenvectors are orthogonal to each other is because the Eigenvectors should be able to span the whole x-y area. As our dummy dataset had 2 dimensions, there will be 2 principal components or eigenvectors.

Thus, determining eigenvectors and eigenvalues will give us the desired new axis or the principal components. Let us now learn how to calculate Eigenvalues and Eigenvectors for our dummy dataset using Python.

---

#### Activity 2: Computing Eigenvalues and Eigenvectors

Let us compute the eigenvalues and eigenvectors of the scaled wheat kernel DataFrame `wheat_scaled` using `numpy` module. For this,
1. Import `linalg` module from `numpy` module.
2. Obtain the covariance matrix of scaled DataFrame using `cov()` function.
3. Pass this covariance matrix as input to `eig()` function of `linalg` module.

The `eig()` function returns the eigenvalues and eigenvectors of the corresponding covariance matrix.
"""

# S2.1: Compute eigenvalues and eigenvectors for scaled wheat kernel DataFrame
from numpy import linalg
# Compute covariance matrix
cov = wheat_scaled.cov()

# Use 'linalg.eig()' function to obtain eigenvectors and eigenvalues
eigen_value,eigen_vector = linalg.eig(cov)
print(eigen_vector)
print()
print(eigen_value)

"""Observe the eigenvalues obtained in the output. As the original dataset had 7 dimensions, we obtained 7 eigenvalues ($\lambda_1$ to $\lambda_7$ ).

Please note that the `np.linalg.eig()` function lists the entries of the eigenvectors column wise.
Thus, the first column of the eigenvector array is the first principal component, second column is the second principal component and so on.

You can also take the transpose of the eigenvector array to obtain eigenvectors row-wise using `T` attribute.
"""

# S2.2: Take transpose of eigenvector array.
transpose_value = eigen_vector.T
transpose_value

"""Thus, in the above output, the first row indicates the first eigenvector or principal component coordinates for 7-dimensions, second row indicates the second eigenvector or principal component and so on.



**Choosing Top $k$ Eigenvectors:**


Recall that the typical goal of a PCA is to reduce the dimensionality of the original feature space.

From the above eigenvectors, we need to decide which eigenvector(s) can be dropped without losing too much information to obtain a lower-dimensional subspace. For this, we need to inspect the corresponding eigenvalues of these eigenvectors.

Let us rank the eigenvalues from highest to lowest to understand which eigenvalue bears the highest information (variance). This can be calculated in the following way:

Variance carried by first principal component:
\begin{align}
\frac{\lambda _1}{\lambda _1 + \lambda _2 + \lambda _3 + \lambda _4 + \lambda _5 + \lambda _6 + \lambda _7} = \frac{5.0252}{7.0362} = 0.71
\end{align}



Variance carried by second principal component:

\begin{align}
\frac{\lambda _2}{\lambda _1 + \lambda _2 + \lambda _3 + \lambda _4 + \lambda _5 + \lambda _6 + \lambda _7} = \frac{1.2442}{7.0362} = 0.18
\end{align}

Similarly, you can determine the amount of information or variance carried by each principal component by obtaining the ratio of variance (eigenvalue / total eigenvalues) i.e.

\begin{align}
\frac{\lambda _i}{\lambda _1 + \lambda _2 + ...+ \lambda _n}  
\end{align}
Where, $\lambda_i$ represents the eigenvalue for the corresponding $i^{th}$ eigenvector. This variance ratio is known as **Explained Variance**.

Let us obtain the explained variance of all 7 principal components by dividing each eigenvalue by the sum of all eigenvalues using Python.

"""

# S2.3: Obtain explained variance of all 7 PCs
import numpy as np
variance = np.round(eigen_value / eigen_value.sum(),2)
variance

"""We can see here that the first principal component comprises 71% of the variation within the data, and thus, most of the information. With each subsequent component, less information is contributed to the compressed data.

It is observed that:

- Most of the variance (approx $71\%$) can be explained by the first principal component alone.
- The second and third principal component still bears some information (approx $18\%$ and $10\%$ respectively).
- The other principal components can be safely dropped without losing too much information.

- Together, the first two principal components explain approx $89\%$ ($71\% + 18\%$ ) of the variance. Hence, if we reduce our 7-dimensional dataset into 2 dimensions, it would retain approx $89\%$ of the information.

- Together, the first three principal components explain approx $99\%$ ($71\% + 18\% + 10\%$ ) of the variance. Hence, if we reduce our 7-dimensional dataset into 3 dimensions, it would retain approx $99\%$ of the information.

Hence, based on how much information you want to retain in the transformed dataset, you can decide the number of dimensions to be reduced.

Recall the steps of PCA that we had studied in **"The Recipe of PCA"** section.

<center>
<img src=https://s3-whjr-v2-prod-bucket.whjr.online/whjr-v2-prod-bucket/c64fbd2d-4017-490a-a568-ad557a3eac30.png> </center>


We have almost performed all the steps of PCA transformation. The last step  is to rotate our data to fit the transformed axis. But, *what will the coordinates of the rotated data be?* Let us find out.

---

#### Activity 3: Transformation to Original Dataset

Once we have found the eigenvectors,  we now convert our original data as per the new axis. This means that, we need to find out the coordinates of the data points when they are rotated to fit this new axis. To understand this, refer to the 3 graphs given in the image below:

<center>
<img src= "https://s3-whjr-v2-prod-bucket.whjr.online/298f99e0-dbd6-4ec8-bc61-99e9378266bf.png"/></center>

- The first graph illustrates the distribution of data along two features or dimensions.
- In the second graph, the data is centered and new axis or principal components are determined.
- In the third graph, the dimensions are now the new transformed axis i.e. `PC1` and `PC2` and the coordinates of the original data points are also transformed.


To rotate the original data onto the new axis, we will multiply the original scaled data by Eigenvectors as follows:

$$\text{Transformed Data = [Scaled Data].[Eigenvectors]}$$

Let us perform  dot multiplication of scaled dataset array with eigenvectors array.

Before that, first determine the number of rows and columns of both the arrays.
"""

# S3.1: Determine the shape of 'wheat_scaled' and 'wheat_eigenvec'
print("Shape of scaled dataset", wheat_scaled.shape)
print("Shape of eigenvector array", eigen_vector.shape)

"""As the number of columns of first array `wheat_scaled` is equal to the number of rows of second array `wheat_eigenvec`, we can perform dot multiplication on both the arrays. Use `np.dot()` function to obtain the dot product of both the arrays."""

# S3.2: Obtain new coordinates of data points for the transformed axis.
transformed_array = np.dot(wheat_scaled,eigen_vector)
transformed_array

"""Here, the final matrix produced is the matrix multiplication of `wheat_scaled` and `wheat_eigenvec` which is a `194 x 7` matrix.

Let us convert the above transformed matrix into a DataFrame and rename the columns.
"""

# S3.3: Convert the array obtained after dot multiplication into a DataFrame
new_df = pd.DataFrame(transformed_array,columns = ['P1','P2','P3','P4','P5','P6','P7'])
new_df

"""You may observe that the above DataFrame obtained after dot multiplication has 194 rows and 7 columns. Each column represents the new coordinates of the 194 data points.
- First column represents the coordinates of 194 data points for first axis (PC1). So, if we need only 1 dimension, we will consider only first column of the above array.
- Similarly, the second column represents the coordinates of 194 data points for the second axis (PC2). So, if we need only 2 dimensions, we will consider only the first two columns of the above array and so on.

Hence, we have transformed our original dataset into a new feature space. Let us visualise the above transformed data.

---

#### Activity 4: PCA 1D projection

Suppose we need to transform our 7-dimensional dataset into 1 dimension. Hence, we would consider only the first column of the transformed matrix obtained after dot multiplication.
"""

# S4.1: Obtain the coordinates of the data points for PC1 axis.
pc1 = new_df['P1']

"""These are the coordinates values of 194 data points for the first axis. Let us visualise these new coordinates values by creating a line plot."""

# S4.2: Visualise 1D projection
plt.figure(figsize=(12,4),dpi= 96)
plt.plot(pc1)
plt.show()

"""Thus, we have reduced the dimensionality of our dataset to 1D by manually performing PCA. The explained variance for PC1 was `0.71`, which indicates that approx 71% of the original data is retained by PC1.

 Let us now perform PCA 1D projection using `sklearn` library and compare its results with manual PCA.

**Using `sklearn` PCA:**

We had already performed PCA using `sklearn` module in one of the previous classes. Let us recall the steps for the same.

1. Import `PCA` from `sklearn.decomposition` module.
2. Pass the number of components/dimensions to the PCA constructor using the following syntax:

  **Syntax of PCA:** `PCA(n_components = None)`

  Where, `n_components` is the number of components to keep. As we are projecting the dataset into  1 dimension, `n_components` would be `1` in this case.

3. Call the `fit_transform()` function on PCA object to obtain the new set of features or principal components.
"""

# S4.3: Transform dataset into 1D using sklearn PCA
from sklearn.decomposition import PCA
pca = PCA(n_components = 1)
transformed_value = pca.fit_transform(wheat_scaled)

"""Let us create a line plot to visualise the above PCA transformed 1D data."""

# S4.4: Visualise sklearn PCA transformed data
plt.figure(figsize=(12,4),dpi = 96)
plt.plot(transformed_value)
plt.show()

"""We can observe that the plot obtained using manual PCA and `sklearn` PCA are the same.

We can also obtain the eigenvectors, eigenvalues and total variance explained by each principal component using the `sklearn` PCA object. For this, use the following methods:

- `pca_object.components_`: To obtain eigenvectors or principal component axis.
- `pca_object.explained_variance_`: To obtain eigenvalues.
- `pca_object.explained_variance_ratio`: To obtain the total variance explained by each principal component.
"""

# S4.5: Print the eigenvectors, eigenvalues and total variance of sklearn PCA object
print(pca.components_)
print(pca.explained_variance_)
print(pca.explained_variance_ratio_)

"""We can observe that the values obtained using `sklearn` module are the same as those obtained by using `eig()` function in **Activity 2: Computing Eigenvalues and Eigenvectors**.

Similarly, let us visualise 2D transformed data using manual and `sklearn` PCA.

---

#### Activity 5: PCA 2D projection


To reduce our 7-dimensional dataset into 2 dimensions, consider the first two columns of the transformed matrix obtained after dot multiplication.
"""

# S5.1: Obtain the coordinates of the data points for PC1 and PC2 axis
two_dim = new_df[['P1','P2']]

"""These are the coordinates values of 194 data points for PC1 and PC2 axis. Let us visualise these new coordinates values by creating a scatter plot. Also, colour code the data points according to the `target` column of the original dataset `clean_df`.




"""

# S5.2: Visualise 1D projection
plt.figure(figsize=(12,4),dpi = 96)
plt.scatter(x = two_dim['P1'],y = two_dim['P2'],c = clean_df['target'])
plt.show()

"""
 Let us now perform PCA 2D projection using `sklearn` library and compare its results with manual PCA."""

# S5.3: Transform dataset into 2D using sklearn PCA
pca2 = PCA(n_components = 2)
pca2_trans = pca2.fit_transform(wheat_scaled)

"""Let us create a scatter plot to visualise the above PCA transformed 2D data."""

# S5.4: Visualise sklearn PCA transformed data
plt.figure(figsize=(12,4))
plt.scatter(pca2_trans[:,0],pca2_trans[:,1],c = clean_df['target'])
plt.show

"""You may observe that the 2D scatter plots obtained using manual and sklearn PCA are not identical. Let us determine the eigenvectors, eigenvalues, and total variance explained by each principal component using `sklearn` module."""

# S5.5: Print the eigenvectors, eigenvalues and total variance of sklearn PCA object.
print(pca2.components_)
print(pca2.explained_variance_)
print(pca2.explained_variance_ratio_)

"""If you compare the eigenvectors obtained using `eig()` function (**Activity 2: Computing Eigenvalues and Eigenvectors**) and using `components_` attribute, you may observe that the second principal component values (second row of eigenvectors array) for both manual and sklearn PCA are identical, but the signs are different. This is just a difference in convention, nothing meaningful. Also note that the negative of eigenvectors is the same eigenvector.

Thus, we have reduced the dimensionality of the wheat kernel dataset using manual PCA, as well as the `sklearn` API.

We will stop here. In the next class, we will start exploring a new problem statement for PCA.

---

### **Project**
You can now attempt the **Applied Tech Project 118 - PCA IV - Eigenvectors and Eigenvalues** on your own.

**Applied Tech Project 118 - PCA IV - Eigenvectors and Eigenvalues**: https://colab.research.google.com/drive/1hGOYmPJUfrTLmFbvlT0YFdXjHip9svqL

---
"""