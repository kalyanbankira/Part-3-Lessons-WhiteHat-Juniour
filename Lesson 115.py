# -*- coding: utf-8 -*-
"""20221214Kalyan - Lesson 115

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1AC7EoCHckfkmMtcmOFfNmbE1aJbO43Pa

# Lesson 115: PCA - Data Visualisation

---

### Teacher-Student Activities

In the previous classes, we learned one of the applications of unsupervised learning known as Clustering. These algorithms partition the dataset into clusters of similar items, thereby discovering hidden patterns in the data.

Going forward, we will start exploring another application of unsupervised learning called **Dimensionality Reduction**. This technique will help us to reduce the number of features (also known as *dimensions*) in the dataset. A popular dimensionality reduction technique that we will explore is **Principal Component Analysis (PCA)**.

In this lesson, we will examine a new problem statement  to understand the use of PCA in data visualisation. Let us look at the problem statement first.

---

#### Problem Statement

We are given a dataset that comprises information on physical properties of wheat kernels belonging to three different varieties of wheat: Kama, Rosa, and Canadian.  

**Wheat** is a grass widely cultivated for its seed.
The **wheat kernel** is the seed from which the wheat plant grows. The figure below shows some physical features of wheat kernel:

<center>
<img src=https://s3-whjr-v2-prod-bucket.whjr.online/d23dce74-ef6e-4b95-939a-b187faa4319a.png width='650'> </center>

This dataset consists of following attributes of a wheat kernel:

|Attribute|Attribute Information|
|-|-|
|`A`|Area ($\text{mm}^2$)|
|`P`|Perimeter (mm)|
|`C`|Compactness is calculated as $\frac{4 \times \pi \times A}{P^2}$|
|`LK`| Length of Kernel (mm)|
|`WK`|Width of Kernel (mm)|
|`A_Coef`|Asymmetry Coefficient (a measure of asymmetry in the kernel shape)|
|`LKG`|Length of Kernel Groove (mm)|
|`target`|Kernel Type (`0` = Kama, `1` = Rosa, `2` = Canadian)|


**Dataset credits:** https://archive.ics.uci.edu/ml/datasets/seeds

**Citation:** Dua, D., & Graff, C.. (2017). UCI Machine Learning Repository.

Let us implement PCA on this dataset to understand and visualise the basic structure, patterns, and relationships of the dataset.

---

#### Activity 1: Import Modules and Read the Data

Let's import the necessary Python modules, read the data from a csv file to create a Pandas DataFrame and go through the necessary data-cleaning process (if required).


Dataset link: https://s3-whjr-curriculum-uploads.whjr.online/52e55558-5ad7-4f93-a854-8186f415bc55.csv
"""

# S1.1: Import the Python modules, read the dataset and create a Pandas DataFrame.


import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Read the dataset
wheat_df = pd.read_csv('https://s3-whjr-curriculum-uploads.whjr.online/52e55558-5ad7-4f93-a854-8186f415bc55.csv')
wheat_df.head()

"""Rename the following columns so that they are easy to interpret using `rename()` function of the DataFrame:
- `A` to `area`
- `P` to `perimeter`
- `C` to `compactness`
- `LK` to `kernel_length`
- `WK` to `kernel_width`
- `A_Coef` to `asymmetry_coefficient`
- `LKG` to `kernel_groove_length`


"""

# S1.2: Rename the columns for better understandability
wheat_df.rename(columns = {'A': 'area', 'P': 'perimeter', 'C': 'compactness',
                            'LK': 'kernel_length', 'WK': 'kernel_width',
                            'A_Coef': 'asymmetry_coefficient', 'LKG':'kernel_groove_length'}, inplace = True)
wheat_df.head()

"""Next, find out the total number of rows and columns, data-types of columns and missing values (if exist) in the dataset."""

# S1.3: Get the total number of rows and columns, data-types of columns and missing values (if exist) in the dataset.
wheat_df.info()

"""The dataset consists of $210$ entries and no missing (or null) values. All the columns are numerical, hence feature encoding is not needed.

Let's perform some basic EDA on the dataset.

---

#### Activity 2: Exploratory Data Analysis

Let us determine whether there are any anomalies or irregularities in the dataset.

**1. Kernel Length vs Kernel Width**

The width of a wheat kernel must always be less than the length of the kernel. Let us extract the instances where `kernel_width` is greater than `kernel_length`.
"""

# S2.1: Check for anomalies in 'kernel_width' and 'kernel_length'.
wheat_df[wheat_df['kernel_width'] >= wheat_df['kernel_length']]

"""As you can observe that there are no instances where `kernel_width` is greater than `kernel_length`.

**2. Kernel Groove Length vs Kernel Length**

The length of kernel groove must be less than the kernel length. Let us find out the instances where `kernel_groove_length` is greater than `kernel_length`.
"""

# S2.2: Check for anomalies in kernel_groove_length and kernel_length.
wheat_df[wheat_df['kernel_groove_length'] >= wheat_df['kernel_length']]

"""Here we see that there are instances where `kernel_groove_length` is greater than `kernel_length`. Although not physically possible this may be due to error/noise in measurement.

Let's remove these anomalies from the dataset and obtain a new DataFrame which consists of only those rows where `kernel_groove_length` is less than `kernel_length`.
"""

# S2.3: Obtain a clean DataFrame
clean_df = wheat_df[wheat_df['kernel_groove_length'] < wheat_df['kernel_length']]
clean_df

"""Now the DataFrame `clean_df` has data points of wheat kernels where there are no physical discrepancies for `kernel_length` and `kernel_groove_length`.

Let's validate the  values in the `compactness` column using the formula provided in the dataset description. To do this, let us first copy our DataFrame `clean_df` to a new DataFrame `validation_df` using the `copy()` function.

**Syntax of `copy()` function:** `DataFrame.copy()`

"""

# S2.4: Create a duplicate copy of the 'clean_df' DataFrame.
d_copy = clean_df.copy()

"""
As per dataset description, the compactness of a wheat kernel is calculated as:

\begin{align}
\frac{4 \times \pi \times A}{P^2}
\end{align}

Where,
- $A$ is the area of the kernel
- $P$ is the perimeter of the kernel
- $\pi$ is a constant value equal to $3.14159$ approx.

Add a new column `compactness_formula` to `validation_df` DataFrame which contains the compactness values of wheat kernels calculated using the above formula.

**Note:** To obtain the $\pi$  value, import  `math` module and use `math.pi` constant."""

# S2.5: Calculate the compactness value using the given formula
d_copy['calculated compactness'] = (4 * np.pi * d_copy['area']) / d_copy['perimeter']**2
d_copy

"""Observe the values of the `compactness` column and `compactness_formula` column in `validation_df` DataFrame. It seems like the values of the `compactness` column are rounded from the formula.

Let's see the instances where the difference in these two columns is more than $0.01$ (considering 1% rounding off error).
"""

# S2.6: Check for anomalies in 'compactness' feature
d_copy[d_copy['compactness'] - d_copy['calculated compactness'] > 0.01]

"""You may observe that there are no data points which exhibit compactness calculation error. Hence, we can say that there are no anomalies in `compactness` column.

As our dataset is now free from all the anomalies and irregularities, let us perform some data visualisation.

---

#### Activity 3: Data Visualisation


Let us now check the feature distribution and outliers.


Use `boxplot()` function of `seaborn` module and pass `target` as $x$-axis values and the other features as $y$-axis values to create boxplots for all the features. Use subplots to display these boxplots.

**Syntax for `boxplot()` function:** `seaborn.boxplot(x = None, y = None, data = None, ax = None)`

Where,
 - `x` and `y` are features of the dataset.
 - `data` is the DataFrame.
 - `ax` is the axes object.
"""

# S3.1: Create boxplots for 'clean_df' columns using subplot.
import seaborn as sns

for i in clean_df.columns:
    plt.figure(figsize=(10,6),dpi = 96)
    sns.boxplot(y = clean_df[i], x = clean_df['target'])
    plt.show()

"""Here we see the majority of the features are distributed throughout the varieties of wheat, and there are no significant outliers for any of the features.

Next, we will try to understand the correlation between variables. For this, compute the correlation matrix among all the variables and plot a heat map.
"""

# S3.2: Let's check the correlation of features
plt.figure(figsize=(14,8),dpi= 96)
sns.heatmap(clean_df.corr(),annot = True)
plt.show()

"""From the above correlation heatmap, we can observe that `asymmetry_coefficient`, `compactness`, and `kernel_width` exhibit moderate correlation  with `target`.

Let us create an interactive 2D scatter plot using the `scatter()` function of `plotly.express` module. Plot highly correlated features `asymmetry_coefficient` and `compactness` on $x$-axis and $y$-axis respectively and colour code the data points with respect to the wheat varieties.


"""

# S3.3: Scatterplot for wheat variety using 2 features
import plotly.express  as px
px.scatter(x = clean_df['asymmetry_coefficient'],y = clean_df['compactness'],color = clean_df['target'])

"""The above scatter plot does not clearly distinguish between different wheat variety. Moreover, this scatter plot does not convey information on other features. Let us use one more feature `kernel_width` which has high correlation with `target` to visualise this dataset.


Create an interactive 3D scatter plot using the `scatter_3d()` function of `plotly.express` module. Plot the three features `asymmetry_coefficient`, `compactness`, and `kernel_width` on  $x$-axis, $y$-axis and $z$-axis respectively and colour code the data points with respect to the wheat varieties.

"""

# S3.4: Scatterplot for wheat variety using 3 features
px.scatter_3d(x = clean_df['asymmetry_coefficient'],y = clean_df['compactness'],color = clean_df['target'],z = clean_df['kernel_width'])

"""From the above 3D scatter plot, you may observe that after adding `kernel_width` dimension, visualisation is now better. The feature `kernel_width` gave us a  good separation of wheat varieties.

*What if we want to add more than 3 features or dimensions to this scatter plot?*

We still have other features like `kernel_length`, `area`, `kernel_groove_length`, and `perimeter` that can help us to improve visualisation and obtain better data interpretation. But, **can you imagine a scatter plot  having more than $3$ dimensions or axis?**


The answer is **no**! Hence, we need a technique that can transform or reduce our  7-dimensional dataset into a low dimensional dataset such as 2D or 3D. Such low dimensional datasets are easier to plot and can be easily interpreted. This technique is known as **Dimensionality Reduction**. There are various dimensionality reduction techniques out of which we will explore only **Principal Component Analysis (PCA)**.

Before diving into PCA, let us understand in more detail why there is a need for dimensionality reduction in machine learning.

---

#### Curse of Dimensionality

**Dimension** refers to the number of features associated with a dataset. Several real-world Machine Learning problems involve hundreds or even tens of thousands of features. Not only does this make training extremely slow, but also finding optimal solution becomes harder. This problem is often referred to as the **curse of dimensionality.**

To avoid the curse of dimensionality, we need a technique to reduce the number of dimensions while keeping the useful information that is provided. This is achieved using dimensionality reduction.

Following are the benefits of performing dimensionality reduction:
1. It reduces the time and storage space required.
2. It becomes easier to visualise the data when reduced to very low dimensions such as 2D or 3D.
3. It is also helpful in the pre-processing or feature engineering stages  as the machine learning model needs to deal only with the most relevant information rather than all features.

Consider a sphere in a 3D space. We can project the sphere into lower 2D space into a circle
with some information loss (the value for the $z$ coordinate) but still retaining much of the
information that describes its original shape. This is exactly what we will achieve by using PCA.

<center>
<img src = "https://s3-whjr-v2-prod-bucket.whjr.online/whjr-v2-prod-bucket/b7fa3b5c-6cc9-498c-815b-1b43c85c8f77.png"/>

`Fig: Transformation of 3D sphere into a 2D space`</center>

Let us first apply PCA using `sklearn` library to the wheat kernel dataset and observe how the 7-dimensional dataset is reduced to lower dimensional data.

---

#### Activity 4: Applying PCA

Principal Components Analysis (PCA) is a dimensionality reduction technique. This algorithm transforms the features of a dataset into a new set of features called **Principal Components**. By doing this, a major chunk of the information of the entire dataset is effectively compressed in fewer columns. We will learn more about principal components in the next class.

Let us first implement PCA using `sklearn` library and  then we will discuss the complete concept behind the PCA algorithm in the later section.

Before applying PCA, let us obtain a new DataFrame containing only feature variables.
"""

# S4.1: Create a DataFrame having only feature variables
# Drop the 'target' variable
wheat_features = clean_df.drop(['target'], axis = 1)
wheat_features.head()

"""Let us normalise the above DataFrame before performing PCA so that all the columns have the same mean and variance."""

# S4.2: Normalise the column values.
# Import 'StandardScaler' from sklearn
from sklearn.preprocessing import StandardScaler


# Make an object for StandardScaler
std = StandardScaler()

# Fit and Transform the DataFrame

fit_trans = std.fit_transform(wheat_features)
wheat_scaled = pd.DataFrame(fit_trans,columns = wheat_features.columns)

"""**PCA projection to 2D:**

Let us now apply PCA on the above scaled DataFrame to project the original 7-dimensional data into 2 dimensions. Follow the steps given below to achieve this:

1. Import `PCA` from `sklearn.decomposition` module.
2. Pass the number of components/dimensions to the PCA constructor using the following syntax:

  **Syntax:** `PCA(n_components = None)`

  Where, `n_components` is the number of components to keep. As we are projecting the dataset into  2 dimensions, `n_components` would be `2` in this case.

3. Call the `fit_transform()` function on PCA object to obtain the new set of features or principal components. These components are nothing but the new set of features or columns obtained after PCA transformation.
"""

# S4.3: Transform dataset into 2D using PCA
# Import 'PCA' module from sklearn


from sklearn.decomposition import PCA

# Perform PCA with n_components = 2
pca = PCA(n_components = 2)
pca_2d = pca.fit_transform(wheat_scaled)
pca_2d

"""As the value of `n_components` attribute is `2`, we obtained a 2D array consisting of new set of features. Let us convert this 2D array into a pandas DataFrame and assign column names to this array."""

# S4.4: Convert 2D array to pandas DataFrame
pc_2d_data = pd.DataFrame(data = pca_2d, columns = ['PC1', 'PC2'])
pc_2d_data

"""Thus, we have successfully transformed our 7-dimensional dataset into a 2D dataset. Let us now create a scatter plot with `PC1` values on $x$-axis and `PC2` values on $y$-axis. Also, colour code the data points with respect to the wheat varieties.



"""

# S4.5: Scatterplot for wheat variety using 2 principal components
px.scatter(x = pc_2d_data['PC1'],y =pc_2d_data['PC2'],color = clean_df['target'])

"""As you can see, we obtained a visualisation of the entire dataset by considering compressed set of features rather than original features. Thus, by using PCA, we  can reduce the dimension of the dataset while also retaining as much information as possible.

**PCA projection to 3D:**

Let us now apply PCA on the above scaled DataFrame to project the original data which is 7-dimensional into 3.  Follow the same steps as done for 2D, except that the value of `n_components` will be `3`.

"""

# S4.6: Transform dataset into 3D using PCA

"""As the value of `n_components` attribute is `3`, we obtained a 3D array consisting of new set of features. Let us convert this 3D array into a pandas DataFrame and assign column names to this array."""

# S4.7: Convert 3D array to pandas DataFrame

"""Thus, we have successfully transformed our 7-dimensional dataset into a 3D dataset. Let us  again create a 3D scatter plot with `PC1` values on $x$-axis, `PC2` values on $y$-axis and `PC3` values on $z$-axis. Also, colour code the data points with respect to the wheat variety.




"""

# S4.8: Scatterplot for wheat variety using 3 features

"""Thus, with PCA we have visualised a high dimensional data using `PC1`, `PC2`, and `PC3` as the axes.

PCA can also be combined with clustering to obtain better visualisation of our clustering result, or simply to understand the pattern in our dataset.

We will stop here. In the next class, we will learn what exactly principal components are, how much information each principal component holds and how to compute them from scratch without using any python packages.

---

### **Project**
You can now attempt the **Applied Tech Project 115 - PCA I - Data Visualisation** on your own.

**Applied Tech Project 115 - PCA I - Data Visualisation**: https://colab.research.google.com/drive/11qEXjdkuU6YN_K2645B9mzU1RcsIo2Qp

---
"""