# -*- coding: utf-8 -*-
"""20221228KAlyan- Lesson 120

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1R9Ozw1hTqEGAb-iU_ZMpWquAH72JwBne

#Lesson 120: Feature Reduction Using PCA and RFE

---

### Teacher-Student Activities

In the previous class, we had done preliminary analysis on the forest cover dataset and performed data cleaning activities to make it ready for feature engineering.

In this class, we will test two algorithms: **PCA** and **RFE** which are commonly used for feature reduction on the given dataset. Thereafter, we will compare the performance for both algorithms.

Before that, let us revisit the forest cover problem statement and recall the activities done in the previous class and start this lesson from **Activity 1: Feature Engineering Using PCA**.

---

####Problem Statement

You are given a dataset consisting of forest cover type for $7$ types of trees that predominantly grow in a particular geographic area. The data is acquired for $4$ wilderness areas located in the Roosevelt National Forest of northern Colorado, United States. (**wilderness** refers to natural environments on earth that have minimum/negligible human interference.

You need to build a classifier model to predict the forest cover type based on geographical features.

**Understanding the Dataset:**

The dataset has $2160$ observations for each of the following $7$ forest cover types (trees). Click the tree name link to know more about the respective tree.

**`Target Label`**: `Cover_Type` (integers `1` to `7`) - Forest Cover Type (Tree Type) designation:

<center>

`1` - [Spruce/Fir tree](https://en.wikipedia.org/wiki/Spruce)

<img src=https://s3-whjr-v2-prod-bucket.whjr.online/f696de10-0a55-4ef0-ae8e-25586e9339f4.jpg width='300'>

Image by: By The original uploader was MPF at English Wikipedia. - Transferred from en.wikipedia to Commons., CC BY-SA 3.0, https://commons.wikimedia.org/w/index.php?curid=2440504

<br>

`2` - [Lodgepole Pine tree](https://en.wikipedia.org/wiki/Pinus_contorta)

<img src=https://s3-whjr-v2-prod-bucket.whjr.online/whjr-v2-prod-bucket/b5243445-5da6-4712-b312-5ee663e51eaa.jpeg width='300'>

Image by: By stereogab - Western Pines, CC BY-SA 2.0, https://commons.wikimedia.org/w/index.php?curid=11341935

<br>

`3` - [Ponderosa Pine tree](https://en.wikipedia.org/wiki/Pinus_ponderosa)

<img src=https://s3-whjr-v2-prod-bucket.whjr.online/whjr-v2-prod-bucket/856d8d81-c01d-4e3c-8045-6ab88f94b9e8.jpeg width='300'>

Image by: By Jason Sturner - Custer State Park, Pahá Sápa (Black Hills), South Dakota, CC BY 2.0, https://commons.wikimedia.org/w/index.php?curid=10487324

<br>

`4` - [Cottonwood/Willow tree](https://en.wikipedia.org/wiki/Willow)

<img src=https://s3-whjr-v2-prod-bucket.whjr.online/whjr-v2-prod-bucket/c3d28806-dedc-4858-b87e-532184115fa6.jpeg width='300'>

Image by: Sb2s3 - Own work, CC BY-SA 4.0, https://commons.wikimedia.org/w/index.php?curid=44184869

<br>

`5` - [Aspen tree](https://en.wikipedia.org/wiki/Aspen)

<img src=https://s3-whjr-v2-prod-bucket.whjr.online/whjr-v2-prod-bucket/4390f2a8-93a7-4b83-8f4a-266ecf0e7d54.jpg width='300'>

Image by: By Doug Dolde at English Wikipedia - Contax 645, 120mm, Leaf Aptus 75S, Public Domain, https://commons.wikimedia.org/w/index.php?curid=21714613

<br>

`6` - [Douglas-fir tree](https://en.wikipedia.org/wiki/Douglas_fir)

<img src=https://s3-whjr-v2-prod-bucket.whjr.online/a4d89f9c-77a7-43c5-a492-0253d64f23e5.jpg width='300'>

Image by: By Dave Powell, USDA Forest Service - This image is Image Number 1210046 at Forestry Images, a source for forest health, natural resources and silviculture images operated by The Bugwood Network at the University of Georgia and the USDA Forest Service., CC BY 3.0 US, https://commons.wikimedia.org/w/index.php?curid=5311562

<br>

`7` - [Krummholz tree](https://en.wikipedia.org/wiki/Krummholz)

<img src=https://s3-whjr-v2-prod-bucket.whjr.online/whjr-v2-prod-bucket/8827ad95-d0e0-480a-be1a-68f6febb8d27.jpeg width='300'>

Image by: John Spooner - flickr.com, CC BY 2.0, https://commons.wikimedia.org/w/index.php?curid=5007578
</center>

The dataset has **54 features** which can broadly be divided into two categories:
1. `Geographical fields` (14 features): These features are based on location and other geographical parameters of the patch.

2. `Soil type` (40 features): These features consists of details on the type of soil found in the patch.

**Geographical Fields:**

1. `Elevation` - Elevation in meters.

2. `Aspect` - Aspect in degrees azimuth*.

3. `Slope` - Slope in degrees.

4. `Horizontal_Distance_To_Hydrology` - Horizontal distance to nearest source  of surface water.

5. `Vertical_Distance_To_Hydrology` - Vertical distance to nearest source of surface water.

6. `Horizontal_Distance_To_Roadways` - Horizontal distance to nearest roadway.

7. `Horizontal_Distance_To_Fire_Points` - Horizontal distance to nearest wildfire ignition points.

8. `Hillshade_9am` (0 to 255 index) - Hillshade index at 9am, summer solstice*.

9. `Hillshade_Noon` (0 to 255 index) - Hillshade index at noon, summer solstice.

10. `Hillshade_3pm` (0 to 255 index) - Hillshade index at 3pm, summer solstice.

- `Wilderness_Area` (4 binary columns, 0 = absence or 1 = presence) - Wilderness area designation where the study has been carried. The wilderness areas are:

  ```
11 - Rawah Wilderness Area
12 - Neota Wilderness Area
13 - Comanche Peak Wilderness Area
14 - Cache la Poudre Wilderness Area
  ```

**Soil Type fields:**

`Soil_Type`: These fields indicate the type of soil found in the respective study area
It consists of 40 binary columns where, 0 = absence or 1 = presence of the respective soil type. . Various **soil types** are:

  ```
1 Cathedral family - Rock outcrop complex, extremely stony.
2 Vanet - Ratake families complex, very stony.
3 Haploborolis - Rock outcrop complex, rubbly.
4 Ratake family - Rock outcrop complex, rubbly.
5 Vanet family - Rock outcrop complex, rubbly.
6 Vanet - Wetmore families - Rock outcrop complex, stony.
7 Gothic family.
8 Supervisor - Limber families complex.
9 Troutville family, very stony.
10 Bullwark - Catamount families - Rock outcrop complex, rubbly.
11 Bullwark - Catamount families - Rock land complex, rubbly.
12 Legault family - Rock land complex, stony.
13 Catamount family - Rock land - Bullwark family complex, rubbly.
14 Pachic Argiborolis - Aquolis complex.
15 unspecified in the USFS Soil and ELU Survey.
16 Cryaquolis - Cryoborolis complex.
17 Gateview family - Cryaquolis complex.
18 Rogert family, very stony.
19 Typic Cryaquolis - Borohemists complex.
20 Typic Cryaquepts - Typic Cryaquolls complex.
21 Typic Cryaquolls - Leighcan family, till substratum complex.
22 Leighcan family, till substratum, extremely bouldery.
23 Leighcan family, till substratum - Typic Cryaquolls complex.
24 Leighcan family, extremely stony.
25 Leighcan family, warm, extremely stony.
26 Granile - Catamount families complex, very stony.
27 Leighcan family, warm - Rock outcrop complex, extremely stony.
28 Leighcan family - Rock outcrop complex, extremely stony.
29 Como - Legault families complex, extremely stony.
30 Como family - Rock land - Legault family complex, extremely stony.
31 Leighcan - Catamount families complex, extremely stony.
32 Catamount family - Rock outcrop - Leighcan family complex, extremely stony.
33 Leighcan - Catamount families - Rock outcrop complex, extremely stony.
34 Cryorthents - Rock land complex, extremely stony.
35 Cryumbrepts - Rock outcrop - Cryaquepts complex.
36 Bross family - Rock land - Cryumbrepts complex, extremely stony.
37 Rock outcrop - Cryumbrepts - Cryorthents complex, extremely stony.
38 Leighcan - Moran families - Cryaquolls complex, extremely stony.
39 Moran family - Cryorthents - Leighcan family complex, extremely stony.
40 Moran family - Cryorthents - Rock land complex, extremely stony.
  ```

[*azimuth](https://en.wikipedia.org/wiki/Azimuth): Azimuth angle gives the compass angle/direction with respect to North which is considered as $0^o$.

[*summer solstice](https://en.wikipedia.org/wiki/Summer_solstice): The longest day of summer (June $21^{st}$) when the Earth poles have maximum tilt towards the Sun.

<br>

**Dataset Credits:** https://archive.ics.uci.edu/ml/datasets/covertype

Dua, D., & Graff, C.. (2017). UCI Machine Learning Repository.

**Donors of database:**

1. Jock A. Blackard (jblackard '@' fs.fed.us)
GIS Coordinator
USFS - Forest Inventory & Analysis
Rocky Mountain Research Station
507 25th Street
Ogden, UT 84401

2. Dr. Denis J. Dean (denis.dean '@' utdallas.edu)
Professor
Program in Geography and Geospatial Sciences
School of Economic, Political and Policy Sciences
800 West Campbell Rd
Richardson, TX 75080-3021

3. Dr. Charles W. Anderson (anderson '@' cs.colostate.edu)
Associate Professor
Department of Computer Science
Colorado State University
Fort Collins, CO 80523 USA

---

#### Importing Modules and Reading Data
"""

# Import the required modules
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler

import warnings
warnings.filterwarnings("ignore")

# Read the dataset and print first five records.
df = pd.read_csv('https://s3-whjr-curriculum-uploads.whjr.online/ccf634ae-55a4-4b49-9ef4-9a137c5c07fe.csv')
df.head()

# Get the total number of rows and columns, data types of columns and missing values (if exist) in the dataset.
df.info()

# Let's drop the Id column and verify the removal from the dataset.
df.drop(['Id'], axis = 1, inplace = True)
df.columns

# Get the counts of tree types to verify the counts given in the dataset
df['Cover_Type'].value_counts()

"""---

#### Exploratory Data Analysis

1. Split the DataFrame into features and target variables.
2. Make a list of continuous features.
3. Create a pivot table with `Cover_Type` as index and the list of continous features as values.
4. Visualize the continuous features using boxplots.
5. Drop all the redundant features.
"""

# Split the features and target into separate DataFrames
features_df = df.copy(deep = True).drop(['Cover_Type'], axis = 1)
covertype_df = df['Cover_Type']

# Obtain a list of continuous features
continuous_features = []
for feature in features_df.columns:
  if features_df[feature].nunique() > 2:
    continuous_features.append(feature)

print("Continuous Features:\n", continuous_features)
print('Count of continuous features:', len(continuous_features))

# Display the mean values of the continuous columns with respect to tree type.
cover_pivot_table = pd.pivot_table(df, index = 'Cover_Type', values = df[continuous_features])
cover_pivot_table

# Obtain boxplot for all continuous features
for feature in continuous_features:
    plt.figure(figsize = (10, 6))
    sns.boxplot(y = features_df[feature], x = covertype_df)
    plt.show()

# Obtain the statistical description for continuous features in the dataset.
df[continuous_features].describe()

# Get the counts of values for 'Soil_Type7' and 'Soil_Type15' features

print("For Soil_Type7 feature:", features_df['Soil_Type7'].value_counts())
print("\nFor Soil_Type15 feature:", features_df['Soil_Type15'].value_counts())

# Drop the redundant features.
features_df.drop(['Soil_Type7','Soil_Type15'], axis = 1, inplace = True)
features_df.shape

"""---

#### Correlation Analysis and Data Normalisation

1. Create a correlation heatmap.
2. Create DataFrames for continuous features and binary features and normalize the values.
"""

# Create a correlation heatmap
corr_df = features_df[continuous_features].join(covertype_df).corr()
plt.figure(figsize = (10, 8))
sns.heatmap(corr_df, annot = True)
plt.show()

# Create separate DataFrames for continuous features and binary features

continuous_features_df = features_df[continuous_features]
binary_features_df = features_df.drop(continuous_features, axis=1)

print('Shape of Continuous features: ', continuous_features_df.shape)
print('Shape of Binary features: ', binary_features_df.shape)

# Normalise the continuous features

standard_scaler = StandardScaler()
scaled_values = standard_scaler.fit_transform(continuous_features_df)
scaled_continuous = pd.DataFrame(scaled_values)
scaled_continuous.columns = continuous_features_df.columns

# Join the scaled continuous features and binary features and verify the DataFrame

scaled_features_df = scaled_continuous.join(binary_features_df)
scaled_features_df

"""Let us now perform feature reduction using the `sklearn` PCA and reduce the dimensionality of the high-dimensional forest cover dataset.

---

#### Activity 1: Feature Engineering Using PCA

First let's proceed with PCA. We can reduce our 52-dimensional dataset to either 2D or 3D, but we want to retain at least 90% of the information or variance in the reduced dataset. Thus, we need to determine how much percentage of variance each principal component carries.

Recall that in `sklearn` PCA, we can use the following attributes  with the `PCA` object (for instance, `pca_obj`):
- `pca_obj.components_`: To determine principal axis in feature space.
- `pca_obj.explained_variance_`: To determine the amount of variance explained by each of the components.
- `pca_obj.explained_variance_ratio_:`  To determine the percentage of variance explained by each of the components.

Use the `explained_variance_ratio_` attribute to determine the percentage of variance explained by all the 52 components. For this,
1. Pass `n_components = 52` to the `PCA` constructor.
2. Call the `fit_transform()` function on the PCA object and pass the scaled dataset `scaled_features_df` as input to this function.
"""

# S1.1: Determine explained_variance_ratio of 52 PCs.
from sklearn.decomposition import PCA
pca_obj = PCA(n_components = 52)
pca_52d = pca_obj.fit_transform(scaled_features_df)
print("Percentage of variance explained by each PC:", np.round(pca_obj.explained_variance_ratio_, 3))

"""In the above output, you can observe that first principal component explains approx $25.4\%$ of variance, second component explains approx $20.6\%$ of variance and so on. Thus, first 2 components explains approx $46\%$ ($25.4\%$ + $20.6\%$) of variance. This $46\%$ is nothing but the **cumulative sum of variances** upto 2 components.

Let us calculate the cumulative variance of 52 principal components using cumulative sum function `cumsum()` of `numpy` module.

"""

# T1.1: Calculate cumulative explained variance ratio
cumulative_exp_var = np.cumsum(pca_obj.explained_variance_ratio_)
cumulative_exp_var

"""Thus, first 2 components hold approx $46\%$ of variance, first 3 components hold approx $61.9\%$ variance and so on.

Let us visualise the above cumulative sum of variances for all 52 principal components by creating a line plot.

"""

# S1.2: Create a line plot with components on x-axis and their cumulative sum on y-axis.
plt.figure(figsize=(16,4),dpi= 96)
plt.plot(range(1,53),cumulative_exp_var)
plt.xticks(np.arange(1,52))
plt.grid()
plt.show()

"""In the above graph, we can observe that $90\%$  of variance is obtained at `n_components = 9`. This means that if we reduce our 52-dimensional dataset into 9 dimensions, it would retain approx  $90\%$  of the information.

Thus, let us now apply `sklearn` PCA on the scaled DataFrame `scaled_features_df` and reduce it to 9 dimensions using the steps given below:



1. Import `PCA` from `sklearn.decomposition` module.
2. Pass the number of components/dimensions to the PCA constructor using the following syntax:

  **Syntax of PCA:** `PCA(n_components = None)`

  Where, `n_components` is the number of components to keep. As we are projecting the dataset into  9 dimensions, `n_components` would be `9` in this case. Also, pass `random_state = 42` to obtain same result on every execution.

3. Call the `fit_transform()` function on PCA object to obtain the new set of features or principal components.

4. Convert the array obtained after applying `fit_transform()` function to a DataFrame.




"""

# S1.3: Reduce the number of features from 52 to 9 using PCA.
pca9 = PCA(n_components = 9)
trans_pca9 = pca9.fit_transform(scaled_features_df)
trans_pca9
pca9_df = pd.DataFrame(trans_pca9)
pca9_df

"""Thus, we have reduced the number of features from 52 to 9 by using PCA thereby retaining approx $90\%$ of information.

Now, let us build a classification model using these 9 features and evaluate the performance of this model.

---

#### Activity 2: Classification Using PCA Reduced Features

For our dataset, the `pca_features_df` DataFrame contains feature variables and `covertype_df` DataFrame contains the target variable.

Let us split the DataFrames into train and test sets for building a classification model. As the target variable `Cover_Type ` has 7 unique values (7 forest cover types), this is a **multi-class classification problem**. In such cases, set `stratify = covertype_df` inside `train_test_split()` function to obtain a balanced train/test dataset consisting of samples from each target label.
"""

# S2.1: Perform train-test split
from sklearn.model_selection import train_test_split
X_train,X_test,y_train,y_test = train_test_split(pca9_df,covertype_df,test_size =  0.3,random_state = 42,stratify = covertype_df)

"""Let's proceed with **Support Vector Classifier** for the classifier design. We will use `GridSearchCV` to obtain the optimal classifier hyperparameters. Let us first recall what is `GridSearchCV` and how to use it.


**Recalling `GridSearchCV`:**

`GridSearchCV` is a library function that is a member of `sklearn.model_selection` package. It helps to loop through predefined hyperparameters and fit your estimator (model) on your training set. So, in the end, you can select the best parameters from the listed hyperparameters.

For detailed information: https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html

The `GridSearchCV` takes the following arguments:

- `estimator`: The estimator instance from scikit learn. In our case this will be `SVC()`.

- `param_grid`: The dictionary object that holds the hyperparameters you want to try.

- `scoring`: The evaluation metric that you want to use, you can simply pass a valid string/ object of evaluation metric.

- `n_jobs`: The number of processes you wish to run in parallel for this task. If it is `-1`,  it will use all available processors.

Follow the steps given below to tune the hyperparameters needed for `SVC()` model:

1. Import `GridSearchCV` class from the `sklearn.model_selection` module.

2. Define dictionary, say `param_grid` to select which parameters from `SVC` class you want to run the optimisation. Let us set:

  - `gamma`: `[1, 0.1]`

  - `kernel`: `['rbf', 'poly', 'sigmoid']`

  - `random_state`: `[42]` (for consistent results)

3. Construct a SVC grid using `GridSearchCV` function with following inputs:

 - `SVC`: The classifier model we want to deploy.

 - `param_grid`: The set of parameters for which classifier performance would be evaluated.

 - `scoring`: Use `'accuracy'` as the scoring criteria.

4. Call the `fit()` function on the `grid` to find the best fit. Pass `X_train1` and `y_train1` as inputs.

5. Print the hyperparameters which exhibit highest score using `grid.best_estimator_`.

**Note:** It may take around $7$ minutes to run the next cell.
"""

# S2.2: Obtain the optimum classifier hyperparameters using GridSearchCV

# Import the required library
from sklearn.svm import SVC
from sklearn.model_selection import GridSearchCV

# Define the parameters grid for optimisation
param_grid = {'gamma': [1, 0.1], 'kernel': ['rbf', 'poly', 'sigmoid']}


# Training
grid_search_cv = GridSearchCV(estimator = SVC(),param_grid = param_grid,scoring = 'accuracy',n_jobs = -1)
grid_search_cv.fit(X_train,y_train)

# Print the best hyperparameters
best_svc = grid_search_cv.best_estimator_
best_svc

"""Next, let's model the problem using the best hyperparameters obtained from `GridSearchCV`. For this:

 1. Create a `pca_svc` object of `SVC` class and pass the hyperparameters obtained from `GridSearchCV`, however set `random_state=42` to get consistent results.

 2. Call the `fit()` function on `pca_svc` object with `X_train1` and `y_train1`

 3. Call the `predict()` function on the `pca_svc` object with `X_test1` as the input parameter.

 4. Evaluate the model accuracy using `accuracy_score()` function.
"""

# S2.3: Train an SVC model using the best hyperparameters.
from sklearn.metrics import accuracy_score
best_svc.fit(X_train,y_train)
predict = best_svc.predict(X_test)
accuracy_score(y_test,predict)

# Fit the SVC model, perform prediction and determine the accuracy using 'accuracy_score()' function.

"""The accuracy is around $80$% which is good accuracy score. However let's plot the confusion matrix and print the classification report to get an in-depth overview of the classifier performance."""

# S2.4: Plot the Confusion Matrix and Classification Report for SVC classfier
from sklearn.metrics import classification_report,confusion_matrix
print(confusion_matrix(y_test,predict))
print(classification_report(y_test,predict))

"""Here we observe that the classifier exhibit good scores and we have low number of misclassified labels.

Next, let's check the accuracy that another feature reduction technique called **Recursive Feature Elimination (RFE)** exhibits for $9$ features.

---

#### Activity 3: Feature Engineering Using RFE

Let us first recall the RFE technique used for reducing the number of features of a dataset.
(Learned in Lesson: **Car Price Prediction using RFE**)

**Recalling RFE:**

RFE (Recursive Feature Elimination) is a feature selection or elimination  technique that fits a model and removes the weakest feature (or features). In RFE features are ranked by the model's `coef_` or `feature_importances_` attributes, and by recursively eliminating a small number of features per loop, RFE attempts to eliminate dependencies and collinearity that may exist in a machine learning model.

RFE requires a specified number of features to keep, however it is often not known in advance how many features are valid. However, in order to compare the performance of RFE for features reduction with PCA, we will reduce the number of features to $9$ as obtained from PCA.

Perform the following steps to perform RFE for SVC classifier:

1. Import the `RFE` class from the `sklearn.feature_selection` module.

2. Create an object of `SVC` class, and set the kernel to `linear`. Let's name this object as `svc`.

3. Create an object of the `RFE` class. Let's name it as `rfe`. The constructor of the `RFE` object takes the object of `SVC` class (i.e. `svc`) and the number of features to be selected using RFE as inputs.

4. Call the `fit()` function on the `RFE` object and pass the features dataset (`scaled_features_df`) and target labels (`covertype_df`) as input to this function.

5. Create an array `rfe_features` to store the column names obtained from RFE using `support_` attribute on `scaled_features_df.columns`.

6. From the column index  stored in `rfe_features`, create a DataFrame for the reduced set of features and finally print the DataFrame.

**Note:** It may take around $2$ to $3$ minutes to run the next cell.
"""

# S3.1: Reduce number of feature to 9 using RFE feature extraction method

from sklearn.feature_selection import RFE

# Set SVC kernel
svc = SVC(kernel = 'linear')
rfe = RFE(svc, n_features_to_select=9)
rfe.fit(scaled_features_df, covertype_df)

# Obtain the feature list reduced by RFE and generate the DataFrame for reduced features
rfe_features = scaled_features_df.columns[rfe.support_]
print("Features obtained using RFE:", rfe_features)
rfe_features_df = scaled_features_df[rfe_features]
rfe_features_df

"""From the above output, we can conclude that:

1. Among the continuous features,  only the `Elevation` feature seems of higher significance as per RFE feature extraction.

2. Among the four `Wilderness_Area`,  the `Wilderness_Area1`, `Wilderness_Area3` and `Wilderness_Area4` are more significant and have a diverse variety of trees.

3. Out of $40$ `Soil_Type` features, `Soil_Type4`, `Soil_Type10`, `Soil_Type12`, `Soil_Type20`, and `Soil_Type24` exhibit sufficient information to classify the growth of diverse tree types.


Thus, the `rfe_features_df` DataFrame now consists of the reduced dataset obtained after applying RFE. Let's calculate the VIF (Variance Inflation Factor) values for these reduced features to check if there exists any multicollinearity in the reduced dataset.
"""

# S3.2: Check for the VIF values for 9 features selected by RFE

# Import the 'variance_inflation_factor' module from 'statsmodels.stats.outliers_influence'

from statsmodels.stats.outliers_influence import variance_inflation_factor

# Generate VIF for the Reduced Features Set
vif = pd.DataFrame()
vif["Features"] = rfe_features_df.columns
vif["VIF"] = [variance_inflation_factor(rfe_features_df.values, i) for i in range(9)]
vif

"""All the VIF values less than $5$ which means that the reduced dataset features does not exhibit multicollinearity.

Next let's proceed with building a classification model using the reduced features obtained using RFE.

---

#### Activity 4: Classification using RFE Reduced Features


Our dataset has now been reduced to `rfe_features_df` DataFrame using RFE and now consists of $9$ features.  Let's perform a train test split on this reduced DataFrame.

Since we are performing **Multi-Label Classification**, set `stratify = covertype_df` inside the `train_test_split()` function to obtain a balanced train/test dataset consisting of samples from each target label.
"""

# S4.1: Perform train-test split
X_train,X_test,y_train,y_test  = train_test_split(rfe_features_df,covertype_df,test_size = 0.3,random_state = 42,stratify = covertype_df)
# Print the shape of train and test sets.

"""Let's proceed with **Support Vector Classifier** for the classifier design. We will use `GridSearchCV` to obtain the optimal classifier hyperparameters.

Perform  the same steps as done for PCA reduced features for hyperparameter tuning (**Activity 2: Classification Using PCA Reduced Features**):
1. Import `GridSearchCV` class from the `sklearn.model_selection` module.

2. Define dictionary, say `param_grid` to select which parameters from `SVC` class you want to run the optimisation. Let us set:

  - `gamma`: `[1, 0.1]`

  - `kernel`: `['rbf', 'poly', 'sigmoid']`

  - `random_state`: `[42]` (for consistent results)

3. Construct a SVC grid `grid` using `GridSearchCV` function with following inputs:

 - `SVC`: The classifier model we want to deploy.

 - `param_grid`: The set of parameters for which classifier performance would be evaluated.

 - `scoring`: Use `'accuracy'` as the scoring criteria.

4. Call the `fit()` function on the `grid` to find the best fit. Pass `X_train2` and `y_train2` as input.

5. Print the hyperparameters which exhibit highest score using `grid.best_estimator_`.

**Note:** It may take around $2$ to $3$ minutes to run the next cell.
"""

# S4.2: Obtain the optimum classifier hyperparameters using GridSearchCV

# Define the parameters grid for optimisation


# Training
grid2 = GridSearchCV(SVC(),param_grid = param_grid,scoring = 'accuracy',n_jobs = -1)
grid2.fit(X_train,y_train)
# Print the best hyperparameters
bestrfe = grid2.best_estimator_
bestrfe

"""Next, let's model the problem using the best hyperparameters obtained from `GridSearchCV`. For this:

 1. Create a `rfe_svc` object of `SVC` class and pass the hyperparameters obtained from `GridSearchCV`, however set `random_state = 42` to get consistent results.

 2. Call the `fit()` function on `rfe_svc` object with `X_train2` and `y_train2`.

 3. Call the `predict()` function on the `rfe_svc` object with `X_test2` as the input paramter.

 4. Evaluate the model accuracy using `accuracy_score()` function.
"""

# S4.3: Support vector classifier for reduced features
bestrfe.fit(X_train,y_train)
predict2 = bestrfe.predict(X_test)
accuracy_score(y_test,predict2)

# Fit the SVC model, perform prediction and determine the accuracy using 'accuracy_score()' function.

"""The accuracy is around $65$% which is certainly less than the classifier built using PCA based reduced features. However, let's plot the confusion matrix and print classification report for the classifier to see the classifier performance in detail."""

# S4.4: Plot the confusion matrix and classification report for SVC classifier
print(confusion_matrix(y_test,predict2))
print(classification_report(y_test,predict2))

"""The confusion matrix illustrates that the misclassified cases for the `rfe_svc` model are more as compared with `pca_svc` model.

Hence, we can conclude that for the given problem statement, the performance of **PCA** is far superior when compared with **RFE**.

---

#### Takeaway

Key takeaway points from dimensionality reduction:



1. Here we observed that for the given dataset **Principal Component Analysis** outperform **Recursive Feature Elimination** and reduces the effective features of the dataset up to a greater number while still giving better accuracy for classifier without overfitting.

2. The **RFE** retains the binary features in their original form and does not convert them into continuous features. However, **PCA** also converts the binary features to continuous. Hence we need to be careful while using PCA for problem statement which consists of binary features.

3. After reducing the features with RFE there might still exist multicollinearity between the selected features, hence **VIF** calculation should be performed after RFE to ensure that the selected features do not exhibit multicollinearity. However, PCA gives us the features which are orthogonal which means PCA inherently solves the problem of multicollinearity.

4. There is **no clear rule** which tells us that which feature reduction technique gives faster and optimum response, therefore the best way of feature reduction is application of various algorithms and verify the performance of these algorithms.

We will stop here. In the next class, we will  explore a new machine learning problem statement and learn a new classification algorithm.

---

### **Project**

You can now attempt the **Applied Tech Project 120 - Feature Reduction Using PCA and RFE** on your own.

**Applied Tech Project 120 - Feature Reduction Using PCA and RFE**: https://colab.research.google.com/drive/1aLhZampYzMy31IpQCmNQuExON7RRkdZG

---
"""