# -*- coding: utf-8 -*-
"""Copy of 20221209Kalyan- Lesson 114

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1azQ6P14vrWH6YAqF_Q0QnvBzvBC16Hrj

#Lesson 114: Hierarchical Clustering -  Evaluation Metrics

---

### Teacher-Student Activities

In the previous class, we learned how to determine whether a given dataset is suitable for clustering or not. We implemented agglomerative Hierarchical clustering on an NGO dataset and obtained three clusters of countries.

In this class, we will visualise and analyse the derived clusters  and also implement the K-Means algorithm on the same dataset. We will also evaluate both the clustering algorithms using certain similarity metrics.


Before that, let us go through the concepts covered in the previous class and begin the class from **Activity 1: Visualising Clusters**

---

#### NGO Problem Statement

An international charitable NGO  raised around $\$$10 million after a few funding programmes. The CEO of the NGO needs to decide which countries are in immediate need of this fund so that these funds are utilised optimally and effectively.

Our job is to cluster the countries using some socio-economic and health factors that determine the overall development of the country and provide suggestions of the countries to the CEO of the NGO.

We will use a dataset of 167 countries consisting of following attributes:

|Attribute|Description|
|-|-|
|`country`|Name of the country.|
|`child_mort` | Death of children under 5 years of age per 1000 live births.|
|`exports`| Exports of goods and services per capita. Given as percentage of the GDP per capita.|
|`health`| Total health spending per capita. Given as percentage of GDP per capita.|
|`imports` | Imports of goods and services per capita. Given as percentage of the GDP per capita.|
|`income` | Net income per person.|
|`inflation` | The measurement of the annual growth rate of the Total GDP.|
|`life_expec` | The average number of years a new born child would live if the current mortality patterns are to remain the same.|
|`total_fer` |The number of children that would be born to each woman if the current age-fertility rates remain the same.|
|`gdpp` | The GDP per capita. Calculated as the Total GDP divided by the total population.|

**Dataset Credits:** https://www.kaggle.com/gauravduttakiit/help-international

---

#### Loading the Dataset





**Dataset Link:**  https://s3-whjr-curriculum-uploads.whjr.online/be99ea2b-cb07-4e52-b9ee-4c7e893ae48d.csv
"""

# Import the Python modules, read the dataset and create a Pandas DataFrame.
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

file_path = "https://s3-whjr-curriculum-uploads.whjr.online/be99ea2b-cb07-4e52-b9ee-4c7e893ae48d.csv"
ngo_df = pd.read_csv(file_path)
ngo_df.head()

# Get the total number of rows and columns, data types of columns and missing values (if exist) in the dataset.
ngo_df.info()

"""There are **167 rows and 10 columns** and no missing values in the dataset. Out of the 10 columns, 9 are numerical and only 1 categorical column is present which is the name of the country.

---

#### Data Preparation and EDA

From the dataset description, we may observe that  the values of  `exports`, `health`, and `imports` columns are expressed in the form of percentage of the GDP (`gdpp`).

Let us change these column values from  percentage of total GDP to actual values, as the percentage values may not give a clear picture of that country.
"""

# Convert column values from percentage to actual values.
ngo_df['exports'] = (ngo_df['exports'] * ngo_df['gdpp']) / 100
ngo_df['health'] = (ngo_df['health'] * ngo_df['gdpp']) / 100
ngo_df['imports'] = (ngo_df['imports'] * ngo_df['gdpp']) / 100
ngo_df.head()

"""Now, create a new DataFrame consisting of only numeric columns. Also we will scale the numerical DataFrame so that all the columns have the same mean and variance to perform clustering."""

# Keep only the numerical columns.
ngo_num = ngo_df.drop(['country'], axis = 1)
ngo_num.head(5)

"""Now, normalize the columns of the DataFrame to scale them in a particular range."""

# Normalise the column values.
# Import StandardScaler module from sklearn
from sklearn.preprocessing import StandardScaler

# Create an object for StandardScaler()
standard_scaler = StandardScaler()

# Fit and transform the DataFrame
scaled_values = standard_scaler.fit_transform(ngo_num)
ngo_scaled = pd.DataFrame(scaled_values)
ngo_scaled.columns = ngo_num.columns
ngo_scaled.head()

"""Now, we will try to understand the correlation between variables. For this, compute the correlation matrix among all the numeric variables and plot a heat map."""

# Create a corrleation heatmap.
corr_df = ngo_scaled.corr()
plt.figure(figsize = (7, 5), dpi = 125)
sns.heatmap(corr_df, annot = True)
plt.show()

"""From the above correlation heatmap, we can observe that:

- `gdpp` and `income` has  high positive correlated with correlation of `0.9`. This means that the countries where people have high income will have high GDP.

- `child_mort` and `life_expec` are negatively correlated with a high correlation of `-0.89`. Thus, child mortality rate greatly impacts the overall life expectancy of the population.

- `child_mort` and `total_fer` are highly correlated with correlation of `0.85`. It may be due to the fact that if child mortality is higher, people may opt for more children.

- `imports` and `exports` are highly correlated with correlation of `0.99`.

- `gdpp` and `health` are highly correlated with correlation of `0.92`.

- `life_expec` and `total_fer` are negatively correlated with a high correlation of `-0.76`.  It may be due to unavailability of  health care system for better care for children as well as care for family planning.

Let us now plot boxplots to understand the distribution of numerical columns and detect whether there are any outliers in the dataset. Use subplots to create these boxplots.

Before plotting boxplots, let us give colours to our boxplots by building a colour palette. To build a colour palette, use `color_palette()` function of `seaborn` module which will return a list of colours defining a palette.

For more detailed syntax of `color_palette()` function, use `help()` function.
"""

# Save a palette to a variable.
bp_palette = sns.color_palette("bright")

# Use palplot and pass in the variable:
sns.palplot(bp_palette)
type(bp_palette)

"""Now, let's create boxplots for the numeric columns in the DataFrame using subplots."""

# Create boxplots for numeric columns using subplot.
fig, axis = plt.subplots(nrows = 3, ncols = 3, figsize = (15, 10), dpi = 100, sharex = False)
# As there are 9 columns, we created subplot having 3 rows and 3 columns.
count = 0

for i in range(0, 3):
  for j in range (0, 3):
    column = ngo_num.iloc[:, count]    # Fetching the current column and all rows of that column.
    sns.boxplot(data = ngo_num, x = column, ax = axis[i, j], color = bp_palette[count])
    count = count + 1

plt.show()

"""We may observe that there is atleast one outlier in all the features. In case of gdpp and health, there are too many outliers.

However, since we have limited number of countries (167 countries), removing these outliers  based on IQR (Inter-quartile range) values would remove few countries that deserved the financial aid. Hence, we would not remove the outliers.

---

#### Determining Cluster Tendency

Before we apply any clustering algorithm to the given dataset, it is essential to determine whether the given data has any meaningful clusters or not. In general, we need to check whether the given dataset is not random.

This process of evaluating whether the dataset is feasible for clustering is known as **clustering tendency**. A well-known test for cluster tendency is the **Hopkins Test**.

**Hopkins test:**

- It is a statistical test that checks if the data follows uniform distribution.
- For example, refer to the following image which illustrates a uniformly distributed dataset.
<center>
    <img src="https://s3-whjr-v2-prod-bucket.whjr.online/58149855-5d35-407d-87f6-452344b69f41.png"/>

    `Image: An example of well-shaped 2D uniformly distributed dataset`

</center>

- Such uniformly distributed dataset is not suitable for clustering.

- If the hopkins score is low (tends to `0`), it means that the data is not uniformly distributed and can be used for clustering.

- If the hopkins score is high (above `0.5`), it means that the data is uniformly distributed and cannot be used for clustering.

**Implementing Hopkins Test using Python:**

- The `pyclustertend` is a Python toolkit for assessing cluster tendency.

- Let us first install `pyclustertend` module using `!pip install`.
"""

# Install 'pyclustertend'
# !pip install pyclustertend

"""Once the `pyclustertend` module have been successfully installed, use  `hopkins()` function to determine the hopkins score for your dataset. The syntax for `hopkins()` function is as follows:

`pyclustertend.hopkins(data_frame, sampling_size)`

Where,
- `data_frame`:  The input dataset.
- `sampling_size`: The sampling size which is used to evaluate the number of DataFrame. This value must be equal to or less than the number of rows of our dataset.
    
    For example, If sampling size is 100, then this function generates 100 random data points which are uniformly distributed and then compares our dataset with these 100 points to determine how much they are similar. If our dataset is very similar to these random uniformly distributed dataset, then we will get a high score otherwise a low score.

This function returns a **hopkins score** of the dataset (between `0` and `1`).  A score tending to `0` express a high cluster tendency and a score around `0.5` express no clusterability.

Let us now perform hopkins test for `ngo_scaled` dataset using a sampling size equal to number of rows of our dataset (i.e. `167`).

**Note:** If you assume a sampling size greater than the number of rows, you will get an error stating that the sampling size is greater than DataFrame size.
"""

# Import hopkins and perform hopkins test for 'ngo_scaled' dataset.
# from pyclustertend import hopkins
# hopkins(ngo_scaled, 167)

"""You may observe that the hopkins score obtained is almost `0`, indicating that our dataset has high cluster tendency.

Let us now proceed with dendrogram plotting followed by agglomerative clustering.

---

#### Agglomerative Clustering

Let us first plot dendrograms for both single linkage and complete linkage and select the one which yields the best result.

Follow the steps given below to plot the dendrogram for single linkage:

1. Import `linkage` and `dendrogram` modules from `scipy.cluster.hierarchy` module.
2. Call `linkage()` function and pass `ngo_scaled` dataset and `method = single` as inputs. Store the returned linkage matrix in a `s_distances` variable.
3.  Plot the dendrogram by calling the `dendrogram()` function and pass the linkage matrix `s_distances` as input to this function.

4. Also, set the `xlabel` and `ylabel` of this dendrogram using `matplotlib.pyplot` module.
"""

# Plot dendrogram for single linkage.
from scipy.cluster.hierarchy import linkage, dendrogram
s_distances = linkage(ngo_scaled, method = "single")

plt.figure(figsize = (24, 9))
plt.xlabel("Data Points")
plt.ylabel("Euclidean Distances")
plt.title("Dendrogram for Single Linkage")
dendrogram(s_distances)
plt.show()

"""The clusters of the single linkage are not truly satisfying. It appears to be placing each outlier in its own cluster.

Let us plot dendrogram for complete linkage and observe the result.
"""

# Plot dendrogram for complete linkage.
c_distances = linkage(ngo_scaled, method = "complete")

plt.figure(figsize=(24, 9))
plt.xlabel("Data Points")
plt.ylabel("Euclidean Distances")
plt.title("Dendrogram for Complete Linkage")
dendrogram(c_distances)
plt.show()

"""The result of complete linkage looks good. From the above plot, we observe that if we draw a horizontal line passing through the longest vertical blue line, it cuts the dendrogram at 3 points and also at 4 points. Hence, the number of clusters can be 3 or 4.

Let us obtain 3 clusters of countries using  agglomerative clustering on complete linkage.
"""

# S4.3: Determine the clusters using agglomerative clustering.
# Import AgglomerativeClustering module from sklearn
from sklearn.cluster import AgglomerativeClustering

# Perform AgglomerativeClustering using n_clusters = 3 and linkage = 'complete'
hc_3 = AgglomerativeClustering(n_clusters = 3, linkage = 'complete')

# Fit and predict the data using DataFrame
hcluster_3 = hc_3.fit_predict(ngo_scaled)
print(hcluster_3)

"""As you can see in the output, our dataset is divided into 3 clusters labelled from `0` to `2`.

Let us add these predicted labels as a new column to original DataFrame `ngo_df`.
"""

# Adding the cluster ID back to the ngo_df DataFrame
ngo_df['H_Label'] = hcluster_3
ngo_df.head()

"""Let us determine the number of countries in each cluster."""

# Checking the country count per cluster
ngo_df['H_Label'].value_counts()

"""Let us now analyse the clusters and identify the one which is in dire need of financial aid.

---

#### Activity 1: Visualising Clusters

From the domain understanding,  we know  that child mortality, income, GDP, and healthcare budget are some important factors that decide the development of any country. Hence, we will proceed with analysing the clusters by comparing how these 4 components (`child_mort`, `income`, `gdpp`, `health`) vary for each cluster of countries.

For this, let us create following four scatter plot using subplots:
1. Scatter plot of `gdpp` vs `child_mort`
2. Scatter plot of `income` vs `child_mort`
3. Scatter plot of `gdpp` vs `income`
4. Scatter plot of `income` vs `health`
"""

# S1.1: Visualise clusters using scatter plots.
# S1.1: Visualise clusters using scatter plots.
import seaborn as sns

fig, axis = plt.subplots(nrows = 2, ncols = 2, figsize = (20, 10), dpi = 100)


sns.scatterplot(x = 'gdpp', y = 'child_mort', hue = 'H_Label', ax = axis[0, 0],
                data = ngo_df, legend = 'full', palette = "Dark2", s = 100)

sns.scatterplot(x = 'income', y = 'child_mort', hue = 'H_Label', ax = axis[0, 1],
                data = ngo_df, legend = 'full', palette = "Dark2", s = 100)

sns.scatterplot(x = 'gdpp', y = 'income', hue = 'H_Label', ax = axis[1, 0],
                data = ngo_df, legend = 'full', palette = "Dark2", s = 100)

sns.scatterplot(x = 'income', y = 'health', hue = 'H_Label', ax = axis[1, 1],
                data = ngo_df, legend = 'full', palette = "Dark2", s = 100)

plt.show()

"""From the above graph, we observe that following three clusters are obtained:
- <b><font color = 'green'>Cluster `0`</b></font>: The countries in these clusters have high child mortality, low GDP and low income. Also, these countries have the lowest healthcare budgets.

- <b><font color = 'darkorange'>Cluster `1`</b></font>: As seen in the graph, there is only 1 country in this cluster that has very high GDP and income, extremely low child mortality and high spending on healthcare.

- <b><font color = 'blueviolet'>Cluster `2`</b></font>: The countries in these clusters have average GDP, low child mortality, good income and moderate spending on healthcare.

Let us analyse the mean and median values of all the features of each cluster by grouping `ngo_df` DataFrame based on cluster labels.


"""

# S1.2: Perform 'groupby' analysis to analyse the mean value of each column.
ngo_df.groupby('H_Label').mean()

# S1.3: Perform 'groupby' analysis to analyse the median value of each column.
ngo_df.groupby('H_Label').median()

"""You may observe a huge difference between mean and median values of the columns for cluster `0`. It may be due to the outliers in the dataset.

From both mean and median values, you may observe that the Cluster `0` countries have the highest child mortality and total fertility and lowest value of other attributes like exports, healthcare budget, income, etc.

Let us create barplots for all the columns of `ngo_df` DataFrame except the `country` column. The $x$-axis of these barplots will show the cluster labels (i.e. `0`, `1`, `2`) and $y$-axis shows the corresponding column value for each cluster label.

Use subplots to display these 9 barplots colour coded with `bp_palette` colour palette.

"""

# S1.4: Create barplot for all numeric columns using subplot.
fig2, axis2 = plt.subplots(nrows = 3, ncols = 3, figsize = (18, 9), dpi = 100, sharex = False)
# As there are 9 columns, we created subplot having 3 rows and 3 columns.
count = 1         # initiliase count to 1 and not 0 to exclude 'country' column.

for i in range(0, 3):
  for j in range (0, 3):
    column = ngo_df.iloc[:, count]    # Fetching the current column and all rows of that column.
    sns.barplot(data = ngo_df, x = 'H_Label', y = column, ax = axis2[i, j], color = bp_palette[count])
    count = count + 1

plt.show()

"""
Based on the graphs above,  we should consider cluster `0` countries for NGO aid, because :
- It has the highest child mortality.
- Lowest exports and imports.
- Lowest income.
- Lowest GDP.
- Lowest health expenditure.
- The highest inflation.
- Comparatively low life expectancy.
- Highest total fertility.

Let us list down top 10 countries that require financial aid on priority by sorting column values of `child_mort`, `income`, `health`, and `gdpp` columns.

For this, apply `sort_values()` function on `ngo_df` DataFrame for cluster label `0`. Sort `child_mort` column value in descending order and remaining three column values in ascending order, as we need countries having higher child mortality as well as lower income, healthcare budget and GDP in top 10.
"""

# S1.5: List top 10 countries of Cluster 0 that are in dire need of financial aid and store it in 'hc_top10' variable.
h_top10 = ngo_df[ngo_df['H_Label'] == 0].sort_values(by = ['child_mort','income','health','gdpp'],ascending = [False,True,True,True]).head(10)
h_top10

"""Here are the list of top 10 countries of Cluster 0 that are in dire need of financial aid.

List top 10 country names only from the dataset.
"""

# S1.6: List down the values of 'country' column of 'hc_top10'

"""Hence, we obtained top 10 countries that require NGO aid on priority using Hierarchical clustering.

Let us apply K-Means clustering on the same dataset and determine which algorithm is giving better results.

---

#### Activity 2: Applying K-Means Clustering

We start by finding the optimal number of clusters for the K-Means algorithm. We will use the Elbow method.


Recall the steps for Elbow method:
1. Compute K-Means clustering for different values of `K` by varying `K` from `1` to `10` clusters for `ngo_scaled` DataFrame.
2. For each K, calculate the total within-cluster sum of squares (WCSS) using `inertia_` attribute of `KMeans` object.
3. Plot the curve of WCSS vs the number of clusters `K`.
"""

# S2.1: Determine 'K' using Elbow method.
from sklearn.cluster import KMeans
wcss = []

clusters = range(1, 11)
# Initiate a for loop that ranges from 1 to 10.
for k in clusters:
    # Inside for loop, perform K-means clustering for current value of K. Use 'fit()' to train the model.
    kmeans = KMeans(n_clusters = k, random_state = 10)
    kmeans.fit(ngo_scaled)
    # Find wcss for current K value using 'inertia_' attribute and append it to the empty list.
    wcss.append(kmeans.inertia_)

# Plot WCSS vs number of clusters.
plt.figure(figsize=(16,8),dpi= 96)
plt.plot(clusters,wcss)
plt.xticks(np.arange(1,11))
plt.show()

"""On observing the elbow curve, we can say that either 3 or 4 clusters would be an optimal choice.

In the case of Hierarchical clustering, we obtained 3 clusters of countries. Likewise, let us consider 3 clusters for K-Means as well so that we can compare the clusters obtained from both the clustering algorithms.

Now, perform K-Means clustering with `n_clusters = 3` parameter and determine the cluster labels.

**Note:** You can use the `fit_predict()` function for K-Means as well as used for the `AgglomerativeClustering` class.
"""

# S2.2: Cluster the 'ngo_scaled' dataset for K = 3
kmeans = KMeans(n_clusters = 3,random_state = 10)
predict1 =kmeans.fit_predict(ngo_scaled)

"""As you can see here, the dataset is divided into $3$ clusters labelled from `0` to `2`.

Let us add these predicted labels as a new column to our original `ngo_df` DataFrame.
"""

# S2.3: Add the cluster ID back to the 'ngo_df' DataDrame
ngo_df['K_Label'] = predict1

"""Count the number of countries in each cluster obtained from K-Means clustering."""

# S2.4: Check the country count per cluster
ngo_df['K_Label'].value_counts()

"""We can see that the number of clusters formed using K-Means clustering and Hierarchical clustering are different.


Let us analyse the clusters obtained from K-Means by comparing how `child_mort`, `income`, `gdpp`, `health` vary for each cluster of countries.

For this, let us create following four scatter plot using subplots:
1. Scatter plot of `gdpp` vs `child_mort`.
2. Scatter plot of `income` vs `child_mort`.
3. Scatter plot of `gdpp` vs `income`.
4. Scatter plot of `income` vs `health`.


"""

# S2.5: Visualise the clusters using scatter plots.
import seaborn as sns

fig, axis = plt.subplots(nrows = 2, ncols = 2, figsize = (20, 10), dpi = 100)


sns.scatterplot(x = 'gdpp', y = 'child_mort', hue = 'K_Label', ax = axis[0, 0],
                data = ngo_df, legend = 'full', palette = "Dark2", s = 100)

sns.scatterplot(x = 'income', y = 'child_mort', hue = 'K_Label', ax = axis[0, 1],
                data = ngo_df, legend = 'full', palette = "Dark2", s = 100)

sns.scatterplot(x = 'gdpp', y = 'income', hue = 'K_Label', ax = axis[1, 0],
                data = ngo_df, legend = 'full', palette = "Dark2", s = 100)

sns.scatterplot(x = 'income', y = 'health', hue = 'K_Label', ax = axis[1, 1],
                data = ngo_df, legend = 'full', palette = "Dark2", s = 100)

plt.show()

"""From the above graph, we can observe that <b><font color = 'purple'>Cluster `2`</b></font> countries are the most vulnerable as they have high child mortality, low GDP and low income. Also, these countries have the lowest healthcare budgets.


Let us analyse the mean and median values of all the features of each cluster by grouping `ngo_df` DataFrame based on cluster labels obtained from K-Means.


"""

# S2.6: Perform 'groupby' analysis to analyse the mean value of each column.
ngo_df.groupby('K_Label').mean()

# S2.7: Perform 'groupby' analysis to analyse the median value of each column.
ngo_df.groupby('K_Label').median()

"""**Note:** Ignore the `H_Label` column values in the above DataFrame.

You may observe the difference between mean and median values of the columns for all the clusters. It may be due to the outliers in the dataset.

From both mean and median values, you may observe that the Cluster `2` countries have the highest child mortality, total fertility, and lowest value of other attributes like exports, healthcare budget, income, etc.

Let us again create barplots for all the columns of `ngo_df` DataFrame except the `country` column.

"""

# S2.8: Create barplots for numeric columns using subplot.
fig2, axis2 = plt.subplots(nrows = 3, ncols = 3, figsize = (18, 9), dpi = 100, sharex = False)
# As there are 9 columns, we created subplot having 3 rows and 3 columns.
count = 1

for i in range(0, 3):
  for j in range (0, 3):
    column = ngo_df.iloc[:, count]    # Fetching the current column and all rows of that column.
    sns.barplot(data = ngo_df, x = 'K_Label', y = column, ax = axis2[i, j], color = bp_palette[count])
    count = count + 1

plt.show()

"""These plots are similar to Hierarchical clustering algorithm plots and hence our analysis is good. Hence, based on the graphs above,  we should consider cluster `2` countries for NGO aid.

Let us list down top 10 countries that require financial aid on priority by sorting column values of `child_mort`, `income`, `health`, and `gdpp` columns.

For this, apply `sort_values()` function on `ngo_df` DataFrame for cluster label `2`, as done for Hierarchical clustering.
"""

# S2.9: List top 10 countries of Cluster 2 that are in dire need of financial aid and store it in 'km_top10' variable.
k_top10 = ngo_df[ngo_df['K_Label'] == 2].sort_values(['child_mort','income','health','gdpp'],ascending = [False,True,True,True]).head(10)
k_top10

# S2.10: List down the values of 'country' column of 'km_top10'
k_top10['country'] == h_top10['country']

"""Let us find out whether the top 10 countries that are in dire need of financial aid are the same or different for both clustering algorithms."""

# S2.10: Compare countries of 'km_top10' with 'hc_top10'.

"""We can see that though the number of clusters obtained using K-Means and Hierarchical clustering are different, both the algorithms provided the same 10 countries which require the aid of NGO.

Now that we have explored two main unsupervised learning algorithms, let us learn a few evaluation metrics for clustering algorithms.

---

#### Activity 3: Evaluating Clustering Algorithms

We had already explored various evaluation metrics like RMSE, MAE, $R^2$, etc. to validate the performance of  various supervised learning algorithms.

However, in the case of unsupervised learning, evaluating the algorithm performance is not very straightforward as we do not have the ground truth.  It is not as trivial as counting the number of errors or the precision and recall as in supervised learning algorithms, due to absence of true labels.

The clustering algorithms are evaluated based on some similarity and dissimilarity measures. A good clustering algorithm will split the dataset into clusters in such a way that:

- The objects in the same cluster are similar or to each other as much as possible.
- The objects belonging to different clusters are highly distinct from each other.

The two most popular evaluation metrics for clustering algorithms are the Silhouette score and DB Index.

**1. Silhouette score:**

We had already explored the Silhouette score in one of the previous lessons. Let us recall that.

- The silhouette coefficient of each data point measures how much that data point is similar or close to its own cluster and how much it is distinct from other clusters.

- The silhouette coefficient of data point $i$ is calculated as:

\begin{align}
  s(i) = \frac{b(i)-a(i)}{\text{max}(b(i),a(i))}
\end{align}

Where,

  - $b(i)$: It is the average distance of the data point $i$ with all the points in the closest cluster to its cluster. It is also known as *Mean Inter-cluster distance*.
  <center><img src="https://s3-whjr-v2-prod-bucket.whjr.online/6a2b08d7-145c-4c9b-957f-9ab9be8dc685.gif"/></center>

  - $a(i)$: It is the average distance of data point $i$ with all other points in the same clusters. It is also known as *Mean Intra-cluster distance*.

  <center>
<img src="https://s3-whjr-v2-prod-bucket.whjr.online/dfa627bd-9097-43df-b54b-0e327c417163.gif"/></center>

After calculating the silhouette coefficient for each point, average it out to get the **Silhouette score**. The Silhouette score  falls within the range `[-1, 1]`. Higher Silhouette scores indicate dense and well-separated clusters.

We can obtain the Silhouette score using the `silhouette_score` class of `sklearn.metrics` module.

> **Syntax:** `sklearn.metrics.silhouette_score(dataset, cluster_labels`)

> Where,
  - `dataset`: The dataset which needs to be clustered.
  - `cluster_labels`: The cluster labels predicted by clustering algorithm.

Let us now calculate the Silhouette score for both the clustering algorithms and determine which algorithm has better score.
"""

# S3.1: Calculate Silhouette score of Hierarchical clustering.
from sklearn.metrics import silhouette_score
silhouette_score(ngo_df.iloc[:,1:-2],ngo_df['H_Label'])

# S3.2: Calculate Silhouette score of K-Means clustering.
silhouette_score(ngo_df.iloc[:,1:-2],ngo_df['K_Label'])

"""You may observe that Hierarchical clustering has a higher Silhouette score as compared to K-Means. This means that the clusters obtained from Hierarchical clustering are dense and well separated, which relates to the standard concept of a cluster.

**2. DB Index**

Another metric for evaluating a clustering algorithm is **Daviesâ€“Bouldin** index (DB index).

- It is defined as the average similarity measure of each cluster with its most similar cluster, where similarity is the ratio of within-cluster distances to between-cluster distances.


\begin{align}
\text{DBI} = \frac{1}{k} \sum_{i = 1}^{k} \underset{i \neq j}{\max} \left\{\frac {\sigma_i + \sigma_j}{\delta(c_i, c_j)}\right\}
\end{align}

Where,
  - $k$ is the number of clusters.
  - $\delta(c_i, c_j)$ is the distance between centroid clusters $c_i$ and $c_j$ i.e. intercluster distance.

  <center><img src="https://s3-whjr-v2-prod-bucket.whjr.online/776000d4-5c61-46ca-b3cd-07ef5db420a1.png"/></center>
  - $\sigma_i$ is the average distance within cluster $i$ i.e. the intracluster distance of Cluster $i$.

  <center><img src="https://s3-whjr-v2-prod-bucket.whjr.online/743a2745-4173-47bd-996e-6f01ca21dfe8.png"/> </center>

Thus, like silhouette coefficient, the DB index captures both the separation and compactness of the clusters.

But unlike silhouette coefficient, as DB index falls, the clustering improves. Lower DB index values indicate that the clusters are far enough and less scattered.


We can obtain the DB index using `davies_bouldin_score()` function of `sklearn.metrics` module.


> **Syntax:** `sklearn.metrics.davies_bouldin_score(dataset, cluster_labels`)

> Where,
  - `dataset`: The dataset to be clustered.
  - `cluster_labels`: The cluster labels predicted by clustering algorithm.

Let us now calculate the DB score for both the clustering algorithms and determine which algorithm has better score.
"""

# S3.3: Calculate DB score of Hierarchical clustering.
from sklearn.metrics import davies_bouldin_score
davies_bouldin_score(ngo_df.iloc[:,1:-2],ngo_df['H_Label'])

# S3.4: Calculate DB score of K-Means clustering.
davies_bouldin_score(ngo_df.iloc[:,1:-2],ngo_df['K_Label'])

"""You may observe that K-Means have a very bad DB index score clearly indicating that the clusters are not well separated and are highly scattered. This was also evident in the scatterplots obtained for K-Means clustering.

Hence, both the scores indicated that the Hierarchical clustering formed better clusters than K-Means for the NGO problem statement.

In the next class, we will explore a new problem statement and learn a dimensionality reduction technique.

### **Project**
You can now attempt the **Applied Tech Project 114 - Hierarchical Clustering III** on your own.

**Applied Tech Project 114 - Hierarchical Clustering III**: https://colab.research.google.com/drive/1RMYTA2mlGIYp3-jWXgOX_mlp-cykEwNG

---
"""