# -*- coding: utf-8 -*-
"""Copy of 2022-11-06_Kalyan_APT_Lesson110

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1GOru8CJ_MDpUJ7YWp5UdDJLWF8esI0YU

#Lesson 110: K-Means Clustering - RFM Analysis

---

#### Customer Segmentation Problem Statement


We have a transactional data set which contains all the transactions occurring between 01/12/2010 and 09/12/2011 for a UK-based and registered non-store online retail. The company mainly sells unique all-occasion gifts. Many customers of the company are wholesalers.

The company wants to segment its customers and determine marketing strategies according to these segments

The dataset consists of the following attributes:

- `InvoiceNo`: Invoice number. Nominal, a 6-digit integral number uniquely assigned to each transaction. If this code starts with letter 'c', it indicates a cancellation.

- `StockCode`: Product (item) code. Nominal, a 5-digit integral number uniquely assigned to each distinct product.

- `Description`: Product (item) name. Nominal.

- `Quantity`: The quantities of each product (item) per transaction. Numeric.

- `InvoiceDate`: Invoice Date and time. Numeric, the day and time when each transaction was generated. The date-time format used here is `yyyy-mm-dd hh:mm:ss`

- `UnitPrice`: Unit price. Numeric, Product price per unit in pound sterling, also known as GBP (Great Britain Pound).

- `CustomerID`: Customer number. Nominal, a 5-digit integral number uniquely assigned to each customer.

- `Country`: Country name. Nominal, the name of the country where each customer resides.



**Dataset Credits:** https://archive.ics.uci.edu/ml/datasets/online+retail

**Citation:** Dr Daqing Chen, Director: Public Analytics group. chend '@' lsbu.ac.uk, School of Engineering, London South Bank University, London SE1 0AA, UK.

---

#### Loading the Dataset

Let's import the necessary Python modules (if not imported yet) and read the data from an excel file to create a Pandas DataFrame.



**Dataset Link:** https://s3-student-datasets-bucket.whjr.online/whitehat-ds-datasets/online-retail-customers.xlsx


**Note:** Since the dataset is a Microsoft Excel file, i.e., in the `xlsx` format, we need to use the `read_excel()` function of the Pandas module.
"""

# Read the dataset and create a Pandas DataFrame.
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

file_path = "https://s3-student-datasets-bucket.whjr.online/whitehat-ds-datasets/online-retail-customers.xlsx"
df = pd.read_excel(file_path)
df.head()

"""Now, let's find out the total number of rows and columns, data-types of columns and missing values (if exist) in the dataset."""

# Get the total number of rows and columns, data-types of columns and missing values (if exist) in the dataset.
df.info()

"""There are **541909 rows and 8 columns** and some columns have missing values in the dataset.  We will deal with these missing values in the upcoming section. Let us first deal with cancelled orders.

As per dataset description,  some of the values in field `InvoiceNo` may start with letter **'C'**, to indicate cancelled orders. To search for the rows in the dataset where the `InvoiceNo` starting with 'C', we will use **Regular Expressions**.

---

#### The `Series.str.contains()` function

The `Series.str.contains()` function  is used to check if a pattern or regex is contained within a string of a pandas series or not.

For example, let us use  `Series.str.contains()` function to find if a regex `i[a-n]` is present in the strings in the following pandas series:

[`'India', 'China', 'Russia', 'Sweden', 'Syrian_Arab'`]
"""

# Look for pattern 'ia' in a pandas series.
import pandas as pd
sr = pd.Series(['India', 'China', 'Russia', 'Sweden', 'Syrian_Arab'])
result = sr.str.contains(pat = 'i[a-n]')

# print the result
print(result)

"""As we can see in the output, the `Series.str.contains()` function has returned boolean values for each string in the series. It is `True` if the passed regex is present in the string, else `False` is returned.

Now we will use regex to search for cancelled orders in our dataset and will perform other data processing operations.

---

#### Activity 1: Removing the Cancelled Orders

Now that we have learned how to use regex expressions to search for a pattern within a string, we will now utilise it to search for cancelled orders in our dataset.

As per dataset description, some of the values in field `InvoiceNo` may start with letter 'C', to indicate cancelled orders. Let us first check the data type of the first row of `InvoiceNo` field.
"""

# S1.1: Check the data type of 'InvoiceNo' field
type(df['InvoiceNo'][0])

"""The `InvoiceNo` field has integer values but to find the cancelled orders using **RegEx**, it has to be converted to string data type.

Use `DataFrame.astype('str')` function to convert the data type of `InvoiceNo` column to string.
"""

# S1.2: Convert 'InvoiceNo' field to string and verify whether the data type is converted or not.
df['InvoiceNo'] = df['InvoiceNo'].astype('str')

type(df['InvoiceNo'][0])

"""The values of the `InvoiceNo` column has now been converted into string. Next let's search for  the `InvoiceNo` that contains `C` using Regular Expressions.


We will use `Series.str.contains()` function to check whether any `InvoiceNo` column values contains `C`. Recall the syntax of  `Series.str.contains()` function.

**Syntax:** `Series.str.contains(pat, flags = 0, regex = True)`

Pass the following values to the above function:

- `df[df['InvoiceNo']]` as `Series`.
- `pat = C`

-  `flags = re.IGNORECASE`: To include both lowercase and uppercase strings in the search.
- `regex = True` to indicate that the pattern `pat = C`  is a regex.

"""

# S1.3: Use regex to find 'C' in the 'InvoiceNo' field
import re
cancel_orders = df[df['InvoiceNo'].str.contains(pat = 'C',flags = re.IGNORECASE)]

"""In the output, you may observe that there are 9288 orders whose `InvoiceNo` contains 'C'. Thus, there are 9288 cancelled orders.

Let us remove these cancelled orders from total orders. For that, first check the total number of orders.
"""

# S1.4: Check total number of orders including cancelled orders.
df['InvoiceNo'].shape[0]

"""Thus, there are total 541909 orders out of which 9288 orders were cancelled.

Let us remove cancelled orders from the dataframe which will give us **532621** (541909 - 9288 = 532621) delivered orders.
"""

# S1.5: Remove canceleled invoices from the dataset
df = df[~(df['InvoiceNo'].str.contains('C',flags = re.IGNORECASE,regex = True))]
df

"""Thus, we obtained a dataframe of 532621 rows consisting of delivered orders. Let us now remove the null values from the dataframe.

---

#### Activity 2: Removing Null Values

Now it's time to check which column has missing or null values. Let us now obtain the total count of null values in each column of `df`.
"""

# S2.1: Obtain the number of missing or null values in df
df.isnull().sum()

"""So, there are null values in the `Description` and `CustomerID` column. Let us obtain the percentage of null values with respect to the total rows in the dataframe."""

# S2.2: Determine the percentage of null values in each column.
round(df.isnull().sum()*100 /df.shape[0],4)

"""Thus, the `CustomerID` column has around 25% null values and `Description` column has around 0.27% null values. Let us simply remove these null value rows from the dataframe."""

# S2.3: Remove the null valued rows.
df.dropna(inplace = True)

"""Let us reconfirm whether there are still any missing or null values in the dataframe."""

# S2.4: Again obtain the number of null values in df.
df.isnull().sum()

"""Our dataframe is now free from null or missing values. Let us check the data type of `CustomerID` column."""

# S2.5: Check the data type of CustomerID column.
import numpy as np
df['CustomerID'].dtype

"""The `CustomerID` column is of float data type. We need to convert it into integer based categorical column as `CustomerID` cannot be a floating point value."""

# S2.6: Convert 'CustomerID' field to integer based categorical column.
df['CustomerID'] = df['CustomerID'].astype('int64').astype('category')

"""Here we can see the `CustomerID` field is now a category based field. Next proceed with clustering for customer segmentation. For customer segmentation we will use a well established approach known as **RFM Analysis**.

---

#### Understanding RFM

RFM stands for Recency, Frequency and Monetary. It is a customer segmentation technique that uses past purchase transactions to divide customers into groups. RFM analysis involves calculating following three factors:
 - **Recency:**  How recently the customer have made their purchase.
 - **Frequency:**  How often customers have made their purchases.
 - **Monetary:** How much money customers have paid for their purchases.

Let us understand these three factors in more detail.

**1. Recency**

- It indicates how recently the customer have made their purchase.
- It will give you the number of days that have passed since last purchase made by a customer.
- For example, if `recency = 10` for a particular customer, it means that the last transaction made by that customer was 10 days before.

Consider the following rows from the given dataset:

<img src="https://s3-whjr-v2-prod-bucket.whjr.online/0ff9de93-0dc3-4880-9698-e193c139b47c.PNG"/>


From the above table, it is clearly visible that the customers having `CustomerID` as 12680 and 12462 have recently ordered some items. Thus, the **recency** of purchase can be easily calculated by inspecting `InvoiceDate` and `CustomerID` columns.

**2. Frequency:**

- It indicates how frequently the customers have made their purchases.
- It will give you the total number of transactions made by a customer.
- For example, if `frequency = 2` for a particular customer, it means that the customer has made 2 purchases in total.

Consider the following rows from the given dataset:

<img src="https://s3-whjr-v2-prod-bucket.whjr.online/b0af1160-4d33-4001-b541-76883ba5c8c7.PNG"/>



By looking at `InvoiceNo` and `CustomerID` column, we can say that:
- `CustomerID` 13081 has made 3 transactions.
- `CustomerID` 17850 has made 1 transaction and `CustomerID` 12462 has made 2 transactions.

Thus, the **frequency** of purchase can be easily calculated by inspecting `InvoiceNo` and `CustomerID` columns.

**3. Monetary**

- It indicates how much money customers have paid for their purchases.
- It will give you the total amount spent by a customer.

Consider the following rows from the given dataset:

<img src= 'https://s3-whjr-v2-prod-bucket.whjr.online/b3937de2-b1ab-4e76-9632-b904088e9628.png'/>


The total amount made by a customer in a transaction can be obtained by multiplying the values of `Quantity` and `UnitPrice` columns.


From the above table, we can say that:

- Total amount spent by `CustomerID` 13081 is: $10 \times 1.65 = 16.5$ GBP (Great Britain Pound).

- Total amount spent by `CustomerID` 17850 is: $ (6 \times 2.55) +(6 \times 3.39) = 15.3 + 20.34 = 35.64$ GBP.

Thus, the **monetary** value of purchase can be easily calculated by inspecting `Quantity`, `UnitPrice` and `CustomerID` columns.

**Summarising RFM:**

Consider customer `A` made 2 transactions in this year: one was 100 days ago with 200 GBP and the other one was 90 days ago with 300 GBP. In this case,
- `recency = 90 days`
- `frequency = 2`
- `monetary = 500 GBP`

<center><img src= https://s3-whjr-v2-prod-bucket.whjr.online/whjr-v2-prod-bucket/77be5f73-8acf-4b24-9cc3-9e17ba989708.png width=750></center>

**Advantages of RFM model in customer analysis:**

- The more recent the purchase, the more responsive the customer is to promotions.

- The more frequently the customer buys, the more engaged and satisfied they are.

- Monetary value differentiates heavy spenders from low spenders.

Let us now obtain the **RFM** model from our customer segmentation dataset.

---

#### Activity 3: RFM analysis

Let us first check first 5 rows of this dataset.
"""

# S3.1: Check the first 5 samples of the dataframe
df.head()

"""**Calculating Monetary:**

Monetary value is how much money a customer spends on purchases.

To calculate monetary value, we first need to calculate the total purchase value for the the customers. This can be obtained by multiplying the values of `Quantity` column by `UnitPrice` column.

Let's add a column `TotalPrice` whose values will be the product of `Quantity` and `UnitPrice` column values to the dataframe.
"""

# S3.2: Obtain the the total purchase amount for the customers
df['total']  = df['Quantity'] * df['UnitPrice']
df.head()

"""Now we have a new column named: `TotalPrice` which indicates the total amount spend by each customer.

For Monetary, calculate the amount for all the purchases made by every individual customer. For this, first let us check how many customers do we have in our dataset.
"""

# S3.3: Obtain the number of unique customers
df['CustomerID'].unique()

"""Here we can see that there 4339 categories which implies that our dataset has 4339 unique customers.
Next step is to calculate the total purchase amount spent by these 4339 customers. For this,

1.  Create a dataframe `monetary_df` consisting of following two columns:
  
  1.1. `CustomerID` column.
  
  1.2. `TotalPrice`  column grouped by unique customers.
      - Set the `as_index = False` to reset the index in the new dataframe.
      - Apply `sum()` function on the grouped dataframe to get the total amount spent by the customer.

2. Rename the column `TotalPrice` to `Monetary` in `monetary_df` dataframe.


"""

# S3.4: Obtain the Monetary information from the dataframe
monetary_df = df[['CustomerID','total']].groupby('CustomerID',as_index = False).sum()
monetary_df.rename(columns = {'total' : 'monetary'},inplace = True)
monetary_df.head()

"""Thus, we have now obtained a dataframe which tells us the total amount spend by each customer. We can easily differentiate between heavy spenders and low spenders using the `monetary_df` dataframe.

Let us get the frequency of purchase for respective customers.

**Calculating Frequency:**

Here, we need to count the frequency of purchase made by 4339 unique customers. As already discussed, `CustomerID` and `InvoiceNo` columns are useful to calculate frequency using the steps given below:

1. Create a new dataframe `frequency_df` consisting of following columns:
  
  1.1. `CustomerID` column.
  
  1.2. `InvoiceNo` column grouped by unique customers.    
    - Set the `as_index = False` to reset the index in the new dataframe.
    - Apply `count()` function on the grouped dataframe to get the total number of invoices for each customer.

2. Rename the column `InvoiceNo` to `Frequency` in `frequency_df` dataframe.


"""

# S3.5: Obtain the Frequency information from the dataframe
frequency_df = df[['CustomerID','InvoiceNo']].groupby('CustomerID',as_index = False).count()
frequency_df.rename(columns = {'InvoiceNo' : 'Frequency'},inplace = True)
frequency_df.head()

"""Thus, we obtained a dataframe which shows the frequency of purchases made by each unique customer.

Let us merge the above 2 dataframes i.e. `monetary_df` and `frequency_df` into a single dataframe. Before that, let us first learn what are the different ways of merging dataframes.

---

#### Activity 4: Merging DataFrames

The `merge()` function of `pandas` module is used to combine two or more dataframes, based on a common feature among them. The merging of dataframes is based on **join** operations. Let us understand the two most commonly used join operations:
1. Inner join.
2. Outer join.

**1. Inner join:**
- This is the most widely used merging technique.
- It returns a dataframe with only those rows that have common characteristics.
- An inner join requires each row in the two joined dataframes to have matching column values.
- This is similar to the intersection of two sets.

<center><img src="https://s3-whjr-v2-prod-bucket.whjr.online/0fe6ee12-b61a-4465-af87-b9144f4c0d9d.png"/></center>

Let us consider a similar problem statement of an online seller in India. Consider a sample dataframe `products_df` which consists of basic product details.

|Product_ID | Product_name | Category	| Price	|
| -- | -- | -- | -- |
| 1001 |	Watch |	Fashion |	299.0 |
| 1002 |	Bag |	Fashion |	1350.5 |
| 1003 |	Shoes |	Fashion |	2999.0 |
| 1004 | Smartphone |	Electronics |	14999.0 |
| 1005 | Books |	Study |	145.0 |
| 1006 |	Cheese |	Grocery |	110.0 |
| 1007 |	Laptop |	Electronics |	79999.0 |

Consider another sample datframe `customer_df` of the same company keeping track of the customers details and their purchase.

| Customer_ID |	Name |	Age |	Product_ID |	Purchased_Product |
| -- | -- | -- | -- | -- |
|1	| Priyanshu	| 20	| 1001	| Watch	|
|2	| Rohit	| 25	| 0	| NA	|
|3	| Apeksha	| 15 | 1006 |	Oil |
|4	| Rohan	| 10	| 0 | NA |
|5	| Karan	| 30	| 1003 |	Shoes |
|6	| Divya	| 65	| 1004 |	Smartphone |
|7	| Abhinav	| 35	| 0 |	NA |
|8	| Isha	| 18	| 0 |	NA |
|9	| Vivek	| 23	| 1007|	Laptop |

Let's say the company needs information of all the products sold online as well as the details of customers who purchased those products. These information can be obtained by merging both the dataframes based on the common column `Product_ID` using inner join.

**Syntax for inner join:**

`pd.merge(df1, df2, on='common_column', how='inner')`

where,
 - `df1` and `df2` are the two dataframes to be merged.

The inner join will return only those rows from both the dataframes that have common `ProductID`.

Let us see how this works. First create both the dataframes and apply inner join to merge both the dataframes
"""

# S4.1: Create 'products_df' DataFrame.

products_df = pd.DataFrame({
    'Product_ID':[1001,1002,1003,1004,1005,1006,1007],
    'Product_name':['Watch','Bag','Shoes','Smartphone','Books','Cheese','Laptop'],
    'Category':['Fashion','Fashion','Fashion','Electronics','Study','Grocery','Electronics'],
    'Price':[299.0,1350.50,2999.0,14999.0,145.0,110.0,79999.0],
    })

products_df

"""Create a new DataFrame to which we will apply inner join."""

# S4.2: Create 'customer_df' DataFrame.

customer_df = pd.DataFrame({
    'Customer_ID':[1,2,3,4,5,6,7,8,9],
    'name':['Priyanshu','Rohit','Apeksha','Rohan','Karan','Divya','Abhinav','Isha','Vivek'],
    'age':[20,25,15,10,30,65,35,18,23],
    'Product_ID':[1001,0,1006,0,1003,1004,0,0,1007],
    'Purchased_Product':['Watch','NA','Oil','NA','Shoes','Smartphone','NA','NA','Laptop'],
   })

customer_df

"""Let's now merge the `products_df` and `customer_df` data frames using the inner join approach."""

# S4.3: Use inner join to merge both dataframes
pd.merge(products_df,customer_df,on = 'Product_ID',how = 'inner')

"""From the output, you may observe that only rows having matching `ProductID` are obtained from both the dataframes.

But what if we need to combine both dataframes such that we can find all the products that are not sold and all the customers who didn’t purchase anything from us. In such case, we will use **outer join** operation.

**Outer Join:**
- An outer join returns a set of records (or rows) that include what an inner join would return along with other rows for which no corresponding match is found in the other table.
- The fields where matching data is missing, nulls are produced.

<center><img src="https://s3-whjr-v2-prod-bucket.whjr.online/260fc528-a6ba-47cf-90b1-2fb207e5377d.png"/></center>

  **Syntax for outer join:**

  `pd.merge(df1, df2, on='common_column', how='outer')`

  where,
  - `df1` and `df2` are the two dataframes to be merged.

Let us perform outer join operation on both the dataframes and observe the results.
"""

# S4.4: Use outer join to merge both dataframes
pd.merge(products_df,customer_df,on = 'Product_ID',how = 'outer')

"""From the above output, you may observe that the `merge()` function returned `NaN` for every column of the dataframe that lacks a matching row.

Let us now use `merge()` function to merge the two dataframes `monetary_df` and `frequency_df` using inner join. Store the merged dataframe in `rfm_df` variable.

"""

# S4.5: Merge 'monetary_df' and 'frequency_df' dataframes.
rfm_df = pd.merge(monetary_df,frequency_df,on = 'CustomerID',how = 'inner')
rfm_df

"""Now that we have obtained monetary and frequency values for RFM analysis, let us proceed with calculating the last factor i.e  `recency` for our dataset.

---

#### Activity 5: Calculating Recency

For recency, we need to calculate the number of days between the present date and the date of last purchase made by each customer. As already discussed, `CustomerID` and `InvoiceDate` columns can be used to obtain  the date of last purchase made by a customer. For this,

1. Create a dataframe `recency_df` consisting of following columns:
  
  1.1. `CustomerID` column.
  
  1.2. `InvoiceDate` column grouped by unique customers.
      - Set the `as_index = False` to reset the index in the new dataframe.
      - Apply `max()` function to get the latest purchase date for each customer.

2. Rename the column `InvoiceDate` to `LastPurchaseDate` in `recency_df` dataframe.
"""

# S5.1: Obtain the last purchase date for each customer
recency_df =  df[['CustomerID','InvoiceDate']].groupby('CustomerID',as_index = False).max()
recency_df.rename(columns = {'InvoiceDate' : 'LastPurchaseDate'},inplace = True)
recency_df.head()

"""Now, we have obtained a dataframe which shows the last purchase date of each customer. But for recency, we need the number of days between last purchase date and the present date (or a reference date).

First, find out the last invoice date in the dataset from the `InvoiceDate` column.
"""

# S5.2: Obtain the last invoice date in the dataset.
pre_date = df['InvoiceDate'].max() + pd.Timedelta('1 day')
pre_date

"""Since the last invoice date is `2011–12–09`, we will consider `2011–12–10` as the present date to calculate recency.

**Note:** You can also consider `2011–12–09` as the present date to calculate recency. However, in that case,  we will obtain recency as `0` for purchases made on `2011–12–09` date.

So let us first add 1 day to the last invoice date to obtain the present date (i.e. `2011–12–10`). To do so, use `Timedelta()` function of `pandas` module as follows:

`pd.Timedelta("1 day")`

**Note:** You can use `help(pd.Timedelta)` function to learn more about the syntax of `Timedelta()` function.
"""

# S5.3: Obtain the present date i.e LastPurchaseDate + 1 day
lst_pur = pre_date - recency_df['LastPurchaseDate']
lst_pur

"""Now, we have both the dates i.e. the last purchase date of each customer (`recency_df['LastPurchaseDate']`) and the present date (`present_date`). We can now easily calculate the number of days that have passed since the customer's  last purchase.

"""

# S5.4: Obtain the days since last purchase made by a customer

"""We obtained `timedelta64` object after subtracting last purchase date from the present date for each customer. (Here, `64` indicates 64 bit integers.)

However, for recency , we are interested only in  number of days that has elapsed since last order. To extract days, use `dt.days` attribute with the above pandas series.
"""

# S5.5: Extract days from datetime using 'dt.days' attribute
recency_days = lst_pur.dt.days
recency_days

"""Thus, we obtained recency for 4339 customers. Let's add these days as a column `Recency` to the `rfm_df` dataframe."""

# S5.6: Add 'recency_days' as column to the merged dataframe 'rfm_df'.
rfm_df['recency'] = recency_days
rfm_df['recency'].head()

"""We now have a dataframe for **RFM** analysis consisting of the necessary fields to carry out the customer segmentation.

In the next class, we will analyse the RFM  dataframe and prepare it for K-Means clustering.

---

### **Project**
You can now attempt the **Applied Tech Project 110 - KMeans Clustering IV** on your own.

**Applied Tech Project 110 - KMeans Clustering IV**: https://colab.research.google.com/drive/1WPHj_1Nkn6qpNSoQNSZ-juhGukQWGcL8

---
"""