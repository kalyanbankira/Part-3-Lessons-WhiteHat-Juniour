# -*- coding: utf-8 -*-
"""20221216Kalyan - Lesson 116

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1XxQw9Re7VphHJtfkINXN4X6maLuz_YBM

# Lesson 116: PCA - Covariance and Vectors

---

#### Teacher-Student Activities

In the previous class, we learned another application of unsupervised learning known as dimensionality reduction. We applied Principal Component Analysis (PCA) on a wheat kernel dataset to reduce the number of features and obtain better plots.

In today's class, we will understand the underlying math behind PCA transformation and explore some important concepts of linear algebra that are used in PCA.

Before that, let us go through the concepts covered in the previous class and begin the class with the topic: **The Recipe of PCA**.

---

#### Wheat Kernel Problem Statement

We are given a dataset that comprises information on the physical properties of wheat kernels belonging to three different varieties of wheat: Kama, Rosa, and Canadian.  

**Wheat** is a grass widely cultivated for its seed.
The **wheat kernel** is the seed from which the wheat plant grows. The figure below shows some physical features of wheat kernel:

<center>
<img src=https://s3-whjr-v2-prod-bucket.whjr.online/d23dce74-ef6e-4b95-939a-b187faa4319a.png width='650'> </center>

This dataset consists of the following attributes of a wheat kernel:

|Attribute|Attribute Information|
|-|-|
|`A`|Area ($\text{mm}^2$)|
|`P`|Perimeter (mm)|
|`C`|Compactness is calculated as $\frac{4 \times \pi \times A}{P^2}$|
|`LK`| Length of Kernel (mm)|
|`WK`|Width of Kernel (mm)|
|`A_Coef`|Asymmetry Coefficient (a measure of asymmetry in the kernel shape)|
|`LKG`|Length of Kernel Groove (mm)|
|`target`|Kernel Type (`0` = Kama, `1` = Rosa, `2` = Canadian)|


**Dataset credits:** https://archive.ics.uci.edu/ml/datasets/seeds


**Citation**: Dua, D., & Graff, C.. (2017). UCI Machine Learning Repository.

---

#### Loading the Dataset

**Dataset Link:**  https://s3-whjr-curriculum-uploads.whjr.online/be99ea2b-cb07-4e52-b9ee-4c7e893ae48d.csv
"""

# Import the Python modules, read the dataset and create a Pandas DataFrame.
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px

# Read the dataset
wheat_df = pd.read_csv('https://s3-whjr-curriculum-uploads.whjr.online/52e55558-5ad7-4f93-a854-8186f415bc55.csv')
wheat_df.head()

# Rename the columns for better understandability
wheat_df.rename(columns = {'A': 'area', 'P': 'perimeter', 'C': 'compactness',
                            'LK': 'kernel_length', 'WK': 'kernel_width',
                            'A_Coef': 'asymmetry_coefficient', 'LKG':'kernel_groove_length'}, inplace = True)
wheat_df.head()

# Get the total number of rows and columns, data types of columns and missing values (if exist) in the dataset.
wheat_df.info()

"""---

#### Exploratory Data Analysis

1. Analysis of data where kernel width is more than kernel length.
2. Check for anomalies in kernel groove length and kernel length.
3. Calculating compactness using area and perimeter.

$Compactness = \frac{4*Î *Area}{Circumference^2}$
"""

# Check for anomalies in 'kernel_width' and 'kernel_length'.
wheat_df[wheat_df['kernel_width'] > wheat_df['kernel_length']]

# Check for anomalies in kernel_groove_length and kernel_length.
print("Number of rows having kernel groove length > kernel length =",
      wheat_df[wheat_df['kernel_groove_length'] > wheat_df['kernel_length']].shape[0], "\n") # You can add this code part later.

wheat_df[wheat_df['kernel_groove_length'] > wheat_df['kernel_length']] # Write only this part of code first. Then add the preceding part later.

# Obtain a clean DataFrame
clean_df = wheat_df[wheat_df['kernel_groove_length'] < wheat_df['kernel_length']]
clean_df

# Create a duplicate copy of the 'clean_df' DataFrame.
validation_df = clean_df.copy()
validation_df.head()

# Calculate the compactness value using the given formula
import math
validation_df['compactness_formula'] = (validation_df['area'] * 4 * (math.pi)) / (validation_df['perimeter'] ** 2)
validation_df

# Check for anomalies in 'compactness' feature
validation_df[validation_df['compactness'] - validation_df['compactness_formula'] > 0.01]

"""---

#### Curse of Dimensionality

**Dimension** refers to the number of features associated with a dataset. Several real-world Machine Learning problems involve hundreds or even tens of thousands of features. Not only does this make training extremely slow, but also finding the optimal solution becomes harder. This problem is often referred to as the **curse of dimensionality.**

To avoid the curse of dimensionality, we need a technique to reduce the number of dimensions while keeping the useful information that is provided. This is achieved using dimensionality reduction.

Following are the benefits of performing dimensionality reduction:
1. It reduces the time and storage space required.
2. It becomes easier to visualise the data when reduced to very low dimensions such as 2D or 3D.
3. It is also helpful in the pre-processing or feature engineering stages  as the machine learning model needs to deal only with the most relevant information rather than all features.

Consider a sphere in a 3D space. We can project the sphere into lower 2D space into a circle
with some information loss (the value for the $z$ coordinate) but still retaining much of the
information that describes its original shape. This is exactly what we will achieve by using PCA.

<center>
<img src = "https://s3-whjr-v2-prod-bucket.whjr.online/whjr-v2-prod-bucket/b7fa3b5c-6cc9-498c-815b-1b43c85c8f77.png"/>

`Transformation of 3D sphere into a 2D space`</center>

Let us first apply PCA using `sklearn` library to the wheat kernel dataset and observe how the 7-dimensional dataset is reduced to lower dimensional data.

---

#### Applying PCA

Principal Components Analysis (PCA) is a dimensionality reduction technique. This algorithm transforms the features of a dataset into a new set of features called **Principal Components**. By doing this, a major chunk of the information of the entire dataset is effectively compressed into fewer columns. We will learn more about principal components in the next class.

Let us first implement PCA using `sklearn` library and  then we will discuss the complete concept behind the PCA algorithm in the later section.

Before applying PCA, let us obtain a new DataFrame containing only feature variables.
"""

# Create a DataFrame having only feature variables
# Drop the 'target' variable
wheat_features = clean_df.drop(['target'], axis = 1)
wheat_features.head()

# Normalise the column values.
from sklearn.preprocessing import StandardScaler

standard_scaler = StandardScaler()
scaled_values = standard_scaler.fit_transform(wheat_features)
wheat_scaled = pd.DataFrame(scaled_values)
wheat_scaled.columns = wheat_features.columns
wheat_scaled.head()

"""**PCA projection to 2D:**

Let us now apply PCA on the above scaled DataFrame to project the original 7-dimensional data into 2 dimensions. Follow the steps given below to achieve this:

1. Import `PCA` from `sklearn.decomposition` module.
2. Pass the number of components/dimensions to the PCA constructor using the following syntax:

  **Syntax:** `PCA(n_components = None)`

  where, `n_components` is the number of components to keep. As we are projecting the dataset into  2 dimensions, `n_components` would be `2` in this case.

3. Call the `fit_transform()` function on PCA object to obtain the new set of features or principal components. These components are nothing but the new set of features or columns obtained after PCA transformation.
"""

# Transform dataset into 2D using PCA
from sklearn.decomposition import PCA
pca = PCA(n_components = 2)
pca_2d = pca.fit_transform(wheat_scaled)

# Convert 2D array to pandas DataFrame
pc_2d_data = pd.DataFrame(data = pca_2d, columns = ['PC1', 'PC2'])
pc_2d_data

# Scatterplot for wheat variety using 2 principal components

fig = px.scatter(pc_2d_data, x = 'PC1', y = 'PC2', color = clean_df['target'],
                 color_continuous_scale = 'bluered')
fig.show()

"""As you can see, we obtained a visualisation of the entire dataset by considering the compressed set of features rather than original features. Thus, by using PCA, we  can reduce the dimension of the dataset while also retaining as much information as possible.

**PCA projection to 3D:**

Let us also apply PCA on the scaled DataFrame to project the original data which is 7-dimensional into 3 dimensions. Follow the same steps as done for 2D, except that the value of `n_components` will be `3`.

"""

# Transform dataset into 3D using PCA
from sklearn.decomposition import PCA
pca = PCA(n_components = 3)
pca_3d = pca.fit_transform(wheat_scaled)

# Convert 3D array to pandas DataFrame
pc_3d_data = pd.DataFrame(data = pca_3d, columns = ['PC1', 'PC2', 'PC3'])
pc_3d_data

# Scatterplot for wheat variety using 3 features

fig = px.scatter_3d(pc_3d_data, x = 'PC1', y = 'PC2', z = 'PC3',
                           color = clean_df['target'], color_continuous_scale = 'bluered')
fig.show()

"""Thus, with PCA we have visualised high dimensional data using `PC1`, `PC2`, and `PC3` as the axes.

PCA can also be combined with clustering to obtain better visualisation of our clustering result, or simply to understand the pattern in our dataset.

---

#### The Recipe of PCA

As you might have understood so far, PCA is all about finding Principal Components for which the steps are as follows:

1. Take only the features (or dimensions) from a DataFrame.

2. Compute the mean for every dimension (or feature) of the whole dataset.

3. Compute the covariance matrix of the whole dataset.

4. Compute eigenvectors and the corresponding eigenvalues. You will learn these concepts in this lesson shortly.

5. Sort the eigenvectors by decreasing eigenvalues and choose $k$ eigenvectors having the greatest eigenvalues to form a $n \times k$ dimensional matrix, say $W$ i.e., $W_{n \times k}$ where $n$ is the total number of features (or dimensions) in a DataFrame.

6. Use this $W_{n \times k}$ eigenvector matrix to transform the samples onto the new subspace.

<center>
<img src=https://s3-whjr-v2-prod-bucket.whjr.online/whjr-v2-prod-bucket/c64fbd2d-4017-490a-a568-ad557a3eac30.png> </center>

Let us now manually execute PCA (without `sklearn` module) and reduce the number of dimensions for scaled wheat kernel dataset.

---

#### Activity 1: Covariance

A **covariance** is a statistical parameter that indicates the relationship between two features (or variables).
 - If an increase in one variable results in an increase in the other variable, then both the variables are said to have a **positive** covariance.
 - If an increase or decrease in one variable results in an opposite change in the other variable, then both the variables are said to have a **negative** covariance.

Now, you might be wondering if this sounds exactly  like **correlation**. Well, not entirely. We will soon find out. Till then, to get a better understanding of both covariances, let's look at its mathematical formula.

Let $x_1$ and $x_2$ be two different features of a DataFrame. Then, mathematically, **covariance** is defined as

\begin{align}
\text{cov}(x_1, x_2) = \frac{1}{N}\sum_{i = 1} ^N (x_{1i} - \bar x_1)(x_{2i} - \bar x_2)
\end{align}

where

- $\bar x_1$ is the mean value of feature $x_1$
- $\bar x_2$ is the mean value of feature $x_2$
- $N$ is the total number of samples in a DataFrame
- $i$ is one of the samples in a DataFrame


Let us determine the covariance between all the features of the wheat kernel DataFrame.

**Step 1:** Get all the features (or dimensions) in an array (or data-frame or matrix) which we have already got and stored in the `wheat_features` variables.
"""

# S1.1: Print the features of wheat kernel DataFrame.
wheat_features

"""Here, $N = 194$,  as there are $194$ samples of the wheat in the above DataFrame.

**Step 2:** Compute the mean of all the features in the above DataFrame.

"""

# S1.2: Compute mean of features
wheat_features.mean()

"""**Step 3:** Compute the covariance matrix for all the features in the `wheat_features` DatFrame.

Instead of using the following formula:

\begin{align}
\text{cov}(x_1, x_2) = \frac{1}{N}\sum_{i = 1} ^N (x_{1i} - \bar x_1)(x_{2i} - \bar x_2)
\end{align}


We will use the `cov()` function of the Pandas module to calculate the covariance values between the features.

"""

# S1.3: Compute covariance matrix for wheat kernel DataFrame
result = wheat_features.cov()
result

"""Notice that the covariance values do not necessarily range between $-1$ to $1$ which is the case with correlation.

Let's create a heatmap for the above covariance matrix.
"""

# S1.4: Create a covariance heatmap
plt.figure(figsize=(6,4),dpi = 96)
sns.heatmap(wheat_features.cov(),annot = True)
plt.show()

"""

We can see that `'area'` and `'perimeter'` columns have the highest positive covariance. This shouldn't be surprising because both area and perimeter of any object are highly dependent on each other. So, let's create a scatter plot between them."""

# S1.5: Create a scatterplot between area and perimeter.
plt.figure(figsize=(10,8),dpi= 96)
plt.scatter(wheat_features['area'],wheat_features['perimeter'])
plt.show()

"""The above scatter plot confirms that as the `area` increases, the `perimeter` also increases and vice versa.

The features `'area'` and `'asymmetry_coefficient'` have the greatest negative correlation (`-0.83`). So, let's create a scatter plot between them.
"""

# S1.6: Create a scatterplot between area and asymmetry coefficient.
plt.figure(figsize=(8,6),dpi= 96)
plt.scatter(wheat_features['area'],wheat_features['asymmetry_coefficient'])
plt.show()

"""When the negative covariance is very high, the slope of the straight line that fits the scatters in a plot, has a negative slope.

The above plot isn't the ideal example to understand negative covariance. Hence, let's create new arrays that will have a negative covariance with each other and then plot them through a scatter plot.
"""

# T1.1: Demonstrate negative covariance
array1 = np.random.randint(low = -100, high = 100, size = 194)
array2 =  - (0.5 * array1 + 3 * wheat_features['area'].values)

plt.figure(figsize = (9, 4), dpi = 108)
plt.xlabel("array1")
plt.ylabel("array2")
plt.scatter(array1, array2)
plt.show()

"""Hence, the above scatterplot demonstrates how negative covariance looks like.

As you can see, much like correlation, covariance too tells the kind of relationship the two variables are having. Let us understand the difference between these two terms.

**Difference Between Covariance and Correlation:**

- **Covariance** indicates the direction of the linear relationship between variables.
- **Correlation**, on the other hand, measures both the strength and direction of the linear relationship between two variables.
- Correlation is a function of the covariance.
- Another key difference between them is the fact that correlation values are standardised (lies between $-1$ and $1$) whereas, covariance values are not.

Mathematically, correlation is defined as

\begin{align}
r = \frac{\sum_{i = 1} ^N (x_{1i} - \bar x_1)(x_{2i} - \bar x_2)}{\sqrt{\sum_{i = 1} ^N (x_{1i} - \bar x_1)^2 \sum_{i = 1} ^N(x_{2i} - \bar x_2)^2}}
\end{align}

Where,

- $r$ is	the correlation coefficient.

- $x_{1i}$ is the value of some $i^{\text{th}}$ sample of the $x_1$ feature in a DataFrame.

- $x_{2i}$ is the value of some $i^{\text{th}}$ sample of the $x_2$ feature in a DataFrame.

- $\bar{x}_1$	is the mean of the values of the $x_1$ feature in a DataFrame.

- $\bar{x}_2$	is the mean of the values of the $x_2$ feature in a DataFrame.

- $N$ is the total number of samples.

Now, on dividing the numerator and denominator of $r$ by $N$, we get

\begin{align}
r = \frac{\frac{1}{N}\sum_{i = 1} ^N (x_{1i} - \bar x_1)(x_{2i} - \bar x_2)}{\frac{1}{N}\sqrt{\sum_{i = 1} ^N (x_{1i} - \bar x_1)^2 \sum_{i = 1} ^N(x_{2i} - \bar x_2)^2}}
\end{align}

The numerator in the above expression becomes $\text{cov}(x_1, x_2)$.

\begin{align}
\Rightarrow r = \frac{\text{cov}(x_1, x_2)}{\frac{1}{N}\sqrt{\sum_{i = 1} ^N (x_{1i} - \bar x_1)^2 \sum_{i = 1} ^N (x_{2i} - \bar x_2)^2}}
\end{align}

Now, consider the denominator. It becomes:

\begin{align}
\sqrt{\frac{\sum_{i = 1} ^N (x_{1i} - \bar x_1)^2 \sum_{i = 1} ^N (x_{2i} - \bar x_2)^2}{N^2}} \
\Rightarrow \sqrt{ \left( \frac{\sum_{i = 1} ^N (x_{1i} - \bar x_1)^2}{N} \right) \left( \frac{\sum_{i = 1} ^N (x_{2i} - \bar x_2)^2}{N} \right) } \
\Rightarrow \sqrt{ \left( \frac{\sum_{i = 1} ^N (x_{1i} - \bar x_1)^2}{N} \right)} \sqrt{\left( \frac{\sum_{i = 1} ^N (x_{2i} - \bar x_2)^2}{N} \right) }
\end{align}

We already know that:

$$\sqrt{ \left( \frac{\sum_{i = 1} ^N (x_{1i} - \bar x_1)^2}{N} \right)}$$

denotes standard deviation in the values of $x_1$ feature

$$\therefore \sigma_{x_1} = \sqrt{ \left( \frac{\sum_{i = 1} ^N (x_{1i} - \bar x_1)^2}{N} \right)}$$

Similarly,

$$\therefore \sigma_{x_2} = \sqrt{ \left( \frac{\sum_{i = 1} ^N (x_{2i} - \bar x_2)^2}{N} \right)}$$

Therefore, the denominator becomes

$$\sigma_{x_1} \times \sigma_{x_2}$$

So, the correlation expression becomes

\begin{align}
r = \frac{\text{cov}(x_1, x_2)}{\sigma_{x_1} \sigma_{x_2}}
\end{align}

In other words, the correlation is actually **covariance between two features divided by the product of their corresponding standard deviations**.

Now that we have computed the covariance matrix for the wheat kernel dataset, we will proceed to the next step of PCA i.e. computing eigenvectors and eigenvalues.

---

#### Vectors

The next step is to calculate the eigenvectors and eigenvalues. But before that you need to learn about vectors.

**Q:** What is a vector?

**A vector is an object that has magnitude as well as direction**. For example consider the following two statements:

*Statement 1: Nikhil is driving his car at a speed of $6$ Km/hr*

*Statement 2: Nikhil is driving his car towards **East** on M.G. Road at a speed of $6$ Km/hr*

Whenever we attach direction to an object it becomes a **vector**. The figure below illustrates statement 2.

<br>

<center> <img src=https://s3-whjr-v2-prod-bucket.whjr.online/dc952b93-c716-492c-ba50-add03a143fc2.png width=500></center>

The figure above represents **Velocity** of the car.

**Q:** What is Velocity?

**A:** Velocity is a measure of how fast something moves in a particular direction.

Hence, the direction becomes an important factor. Speed is only half the information on velocity and direction is the other half.

The <b><font color ='maroon'>velocity arrow</font></b> (red coloured arrow) or the Velocity vector indicates the direction of the vehicle and the length of the respective arrow indicates the magnitude (i.e. speed of $6$ km/hr).

Hence we can picture a vector as a directed line segment, the length of which determines the magnitude of the vector and arrow indicates the direction. The direction of the vector is always from its tail to its head.

<center><img src=https://s3-whjr-v2-prod-bucket.whjr.online/758d2ad5-9009-4c38-a341-9575baedec35.png width=500></center>

**Tail** is the starting point of the vector and **Head** is the terminating point of the vector.

<br>

**Vectors in 3-D space:**

Consider an aeroplane taking off from Mumbai airport and flying to New Delhi airport. Here we need to specify $x, y,$ and $z$ coordinates in order to specify its location and velocity.

<center> <img src=https://s3-whjr-v2-prod-bucket.whjr.online/b4e1dcb2-85ab-4a57-aab0-0b132fbadf66.png width=1000></center>

As we know Mumbai is located in the southern part of India relative to New Delhi, we can say that the plane has to fly back to North in order to reach New Delhi airport. During take off say,  the target is to reach a certain height represent by point $B$ from origin which is at Mumbai Airport.

---

#### Activity 2: Vector Representation

Let's see how to define vectors. In above aeroplane example, we had an intermediate point $B$ in the 3-D space, the coordinates of which are:

\begin{align}
B = (-5,1.5,1)
\end{align}

If we draw a vector to this point from origin $A$, the vector can be described as:

\begin{align}
\vec{AB} = \begin{bmatrix}-5\\ 1.5\\ 1 \end{bmatrix}
\end{align}

**Note:** The $\rightarrow$ on $AB$ indicates that the object $AB$ is a vector.

Now let's define this vector by creating a NumPy array.
"""

# S2.1: Create a 3D vector
vect = np.array([-5,1.5,1])
vect

"""Here we see a vector in a 3-dimensional space. We can easily visualise the vectors in $2$ or $3$ dimensions, however vectors can exist in higher dimensions as well.

When we have more than $3$ features in our dataset,  the elements in the array will also increase. For example, considering the wheat kernel dataset:

<center><img src=https://s3-whjr-v2-prod-bucket.whjr.online/2a82f726-ccea-45cd-91ce-b2d8edce8535.png width=900></center>

If we represent row $1$ in vector form it would be a 7-dimensional vector. It can be represented as:

\begin{align}
\vec {X_1} = \begin{bmatrix}14.88\\ 14.57\\ 0.8811\\ 5.554\\ 3.333\\ 1.018\\ 4.956\end{bmatrix}
\end{align}

Let's define this 7-dimensional vector by again creating a NumPy array.

"""

# S2.2: Create a 7-dimensional vector.
new_vector = np.array([14.88,14.57,0.8811,5.554,3.333,1.018,4.956])
new_vector

"""So far the vectors we have seen starts from the origin and hence the coordinates becomes the vector. *But what if the starting point is not the origin?*

 If the starting point of the vector is point $A(x_A, y_A)$ and the end point is $B(x_B, y_B)$, then the vector $\vec{AB}$ is represented as:

\begin{align}
\vec{AB} &= \vec{B} - \vec{A} \\
&= (x_B - x_A, y_B - y_A)
\end{align}

Let us take a simple problem statement to understand this:

**Statement**: You start from your home, pick up some burgers from McDonalds and go to your friend's home for a party.

Let us represent this using vectors.

<center> <img src=https://s3-whjr-v2-prod-bucket.whjr.online/whjr-v2-prod-bucket/bf75e1b4-5af7-49b2-b081-a8618f34f4c9.png width=650></center>

The image above describes the problem statement. The coordinates for various locations are:

1. Home $(x_h, y_h) = (0,0)$

2. McD $(x_m, y_m) = (6, 1.5)$

3. Party Location $(x_p, y_p) = (4, 4)$

The vector for Home to McD is calculated as:

\begin{align}
\vec{hm} &= \text{McD} - \text{Home} \\
&= (x_m - x_h, y_m - y_h) \\
&= \begin{bmatrix} 6 - 0 \\ 1.5 - 0 \end{bmatrix} \\
&= \begin{bmatrix} 6\\ 1.5\end{bmatrix}
\end{align}

Similarly, the vector for McD to Party Location is calculated as:

\begin{align}
\vec{mp} &= \text{Party Location} - \text{McD} \\
&= (x_p - x_m, y_p - y_m) \\
&= \begin{bmatrix}4 - 6 \\ 4 - 1.5 \end{bmatrix} \\
&= \begin{bmatrix} -2\\ 2.5\end{bmatrix}
\end{align}

The negative sign indicates that you have to ride the motorbike into another direction to reach your destination.

The same can be calculated using `numpy` module. For this, first define the coordinates for Home, McD, and Party Location as follows:

- `home = array([0, 0])`

- `mcd = array([6, 1.5])`

- `party = array([4, 4])`

Next obtain the respective vectors by subtracting the starting-point coordinates from the ending-point coordinates, Hence:

- $\vec{hm}$ will be calculated as `mcd - home`

- $\vec{mp}$ will be calculated as `party - mcd`

"""

# S2.3: Obtaining Vectors from Coordinates

# Define the coordinates
home = np.array([0, 0])
mcd = np.array([6, 1.5])
party = np.array([4, 4])

# Obtain the respective Vectors
hm = mcd - home
mp = party - mcd
print(" Home to Mac ",hm)
print(" Mac to party ",mp)

"""Hence, we calculated the vector $\vec{hm}$  and $\vec{mp}$ by subtracting the starting-point coordinates from the ending-point coordinates.

---

**Vector Magnitude**

Now that we know about vector representation let us see how to calculate magnitude (also referred to as **norm**) of a vector. Recall the vector representation:

<center><img src=https://s3-whjr-v2-prod-bucket.whjr.online/758d2ad5-9009-4c38-a341-9575baedec35.png width=500></center>

Consider a vector starting from point $A = (x_A, y_A)$ and the terminating at point $B = (x_B, y_B)$. The magnitude/norm of a vector $\vec{AB}$ is defined as:

\begin{align}
|\vec{AB}| = \sqrt{(x_B - x_A)^2 + (y_B - y_A)^2}
\end{align}

Similarly consider the above vector $\vec{hm}$:

\begin{align}
\vec{hm} = \begin{bmatrix} 6\\ 1.5\end{bmatrix}
\end{align}

The magnitude of vector $\vec{hm}$ will be calculated as:

\begin{align}
|\vec{hm}| =& \sqrt{(x)^2 + (y)^2} \\
&= \sqrt{6^2 + 1.5^2}  \\
&= \sqrt{36 + 2.25} \\
&= \sqrt{38.25} \\
&= 6.1846
\end{align}

<br>

Similarly, consider an $n^{th}$ dimensional vector represented as:

\begin{align}
\vec V = \begin{bmatrix} x_1 \\ x_2 \\ .. \\ .. \\ .. \\ x_n\end{bmatrix}
\end{align}

The norm of vector $\vec V$ will be calculated as:

\begin{align}
|\vec V| = \sqrt{{x_1}^2 + {x_2}^2 + ..... +{x_n}^2}
\end{align}

For example, the 7-dimensional vector $\vec {X_1}$ represented by:

\begin{align}
\vec {X_1} = \begin{bmatrix} x_1 \\ x_2 \\ x_3 \\ x_4 \\ x_5 \\ x_6 \\ x_7 \end{bmatrix} =  \begin{bmatrix}14.88\\ 14.57\\ 0.8811\\ 5.554\\ 3.333\\ 1.018\\ 4.956\end{bmatrix}
\end{align}

The norm of the vector $\vec {X_1}$ will be defined as:

\begin{align}
|\vec {X_1}| &= \sqrt{{x_1}^2 + {x_2}^2 + {x_3}^2 + {x_4}^2 + {x_5}^2 + {x_6}^2 + {x_7}^2} \\
&= \sqrt{14.88^2 + 14.57^2 + 0.8811^2 + 5.554^2 + 3.33^2 + 1.018^2 + 4.956^2} \\
&= \sqrt{221.4144 + 212.2849 + 0.776 + 30.847 + 11.11 + 1.036 + 24.562} \\
&= \sqrt{502.0303} \\
&= 22.406
\end{align}

<br>

The `linalg.norm()` function of `numpy` is used to calculate the norm of vectors. Let us calculate the norm of vector `X1` using this function.
"""

# S2.4: Calculate the norm of  vector X1
lin_alg = np.linalg.norm(new_vector)
lin_alg

"""---

**Components of a Vector**

Consider a 2-dimensional vector $\vec u$:

\begin{align}
\vec u = \begin{bmatrix} u_x \\ u_y \end{bmatrix} = \begin{bmatrix} 5 \\ 5 \end{bmatrix}
\end{align}

<center><img src=https://s3-whjr-v2-prod-bucket.whjr.online/13368db4-600d-4571-b20c-e5ca1bc9418d.png width=500></center>

Let's say we are riding our bike along the vector $\vec u$ in the above figure. This means the effective direction in which we are riding will be **North-East**.  This indicates that we would also be moving:

- In the East direction - indicated by $\vec {U_x}$.

- At the same time, in the North direction - indicated by $\vec {U_y}$.

The $\vec {U_x}$ and $\vec {U_y}$ are known as **vector components** or **projections**. Any given $n$-dimensional vector can always be represented by its $n$ components/projections.

**Vector Angle:**

The vector angle gives us a sense of direction for any given vector. In the above figure, the vector angle is indicated as $\theta$. It is the angle a vector makes with the horizontal axis ($x$-axis).

The vector angle $\theta$ can easily be calculated with the help of trigonometry ratios:

\begin{align}
\text{tan} \space \theta = \frac{\text{Perpendicular}}{\text{Base}}
\end{align}

For the vector $\vec u$ this can be written as:

\begin{align}
\text{tan} \space \theta &= \frac{|\vec {U_y|}}{|\vec {U_x}|} \\
&= \frac{5}{5} \\
&= 1 \\
\Rightarrow \theta &= 45^o
\end{align}

<br>

We can also define a vector with the help of its **magnitude** and **angle**.

For the vector $\vec u$, the magnitude be:

\begin{align}
|\vec u| = \sqrt{5^2 +5^2} = \sqrt {25+25} = \sqrt{50} = 7.07
\end{align}

**The vector $\vec u$ can also be represented as:**

\begin{align}
\vec u = \begin{cases} |\vec u| = 7.07 \\ \theta = 45^o \end{cases}
\end{align}


**Q**: How to calculate components of vectors if you are given the above representation?

**A**: We can calculate the components of vectors using trigonometric ratios.

**Trigonometric Ratios:**
The Trigonometry ratios of an angle $\theta$ in a right-angled triangle are defined as:

\begin{align}
\text{sin} \space \theta &= \frac{\text {Perpendicular}}{\text {Hypotenuse}}
\end{align}

\begin{align}
\text{cos} \space \theta &= \frac{\text {Base}}{\text {Hypotenuse}}
\end{align}

For the angle $\theta$ in the above image,

- $\text{Perpendicular} = \vec{u_y}$
- $\text{Base} = \vec{u_x}$
- $\text{Hypotenuse} = \vec{u}$

Thus,

\begin{align}
\text{sin} \space \theta &= \frac{|\vec u_y|}{|\vec u|}
\end{align}

Similarly,

\begin{align}
\text{cos} \space \theta &=  \frac{|\vec u_x|}{|\vec u|}
\end{align}

Hence, if we are given with magnitude of a vector ($|\vec u|$) and angle ($\theta$), we can find the component of the vectors ($|\vec u_x|$ and $|\vec u_y|$) using the above trigonometric ratios.

We will stop here. In the next class, we will learn some vector operations and will proceed to understand Eigenvectors and Eigenvalues.

---

### **Project**
You can now attempt the **Applied Tech Project 116 - PCA II - Covariance and Vectors** on your own.

**Applied Tech Project 116 - PCA II - Covariance and Vectors**: https://colab.research.google.com/drive/1-HgTvKwD8bbqbM2EnTAE1dQOx4uSU4iv

---
"""